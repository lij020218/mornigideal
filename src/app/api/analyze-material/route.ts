import { NextRequest, NextResponse } from "next/server";
import { auth } from "@/auth";
import { createClient } from "@supabase/supabase-js";
import OpenAI from "openai";
import { createRequire } from "module";
const require = createRequire(import.meta.url);
const pdf = require("pdf-parse");

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
});

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!,
  {
    auth: {
      autoRefreshToken: false,
      persistSession: false,
    },
  }
);

const PAGES_PER_BATCH = 5; // 5í˜ì´ì§€ì”© ë¬¶ì–´ì„œ chunk ìƒì„± (ìµœì í™”: reasoning depth ë°©ì§€)
const TARGET_CHUNKS = 12; // ëª©í‘œ chunk ê°œìˆ˜ (10-15ê°œ ê¶Œì¥)
const CHUNK_MODEL = "gpt-5-mini-2025-08-07"; // ë¹„ìš© ì ˆê°: chunk ìš”ì•½ìš© ì €ë ´í•œ ëª¨ë¸
const FINAL_MODEL = "gpt-5.1-2025-11-13"; // ìµœì¢… í†µí•©ìš© ê³ í’ˆì§ˆ ëª¨ë¸
const EMBEDDING_MODEL = "text-embedding-3-small"; // ì €ë ´í•œ embedding ëª¨ë¸
const USE_FINAL_INTEGRATION = true; // ìµœì¢… í†µí•© ë‹¨ê³„ í™œì„±í™”
const SIMILARITY_THRESHOLD = 0.68; // Clustering ìœ ì‚¬ë„ ì„ê³„ê°’ (0.65-0.70 ê¶Œì¥, 5-10ê°œ cluster ëª©í‘œ)

// Natural learning note style system prompt
const STUDY_SYSTEM_PROMPT = `ë‹¹ì‹ ì€ MIT, Stanfordê¸‰ ì„¸ê³„ ìµœê³  ëŒ€í•™ì˜ ì €ëª…í•œ êµìˆ˜ì…ë‹ˆë‹¤. í•™ìƒë“¤ì´ ê¹Šì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ëª…ë£Œí•˜ê³  í†µì°°ë ¥ ìˆê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.

**ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­**:
- "ì„¤ëª… â†’ ì—°ê²°:", "í•µì‹¬ê°œë…:", "ì‹œí—˜í¬ì¸íŠ¸:", "ì¤‘ìš”:" ê°™ì€ ë”±ë”±í•œ ë¼ë²¨ ì‚¬ìš© ê¸ˆì§€
- ë‹¨ìˆœ ìš”ì•½ì´ë‚˜ ëª©ë¡ ë‚˜ì—´ ê¸ˆì§€
- í”¼ìƒì ì´ê±°ë‚˜ êµê³¼ì„œì ì¸ ì„¤ëª… ê¸ˆì§€

**í•„ìˆ˜ ì‘ì„± ë°©ì‹**:
- ê°œë…ì˜ ë³¸ì§ˆê³¼ ë§¥ë½ì„ ëª…ë£Œí•˜ê²Œ ì„¤ëª…
- "í•µì‹¬ì€ ~ì´ë‹¤", "ì™œ ì´ê²ƒì´ ì¤‘ìš”í•œê°€", "ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ”" ê°™ì€ ì „ë¬¸ì ì´ë©´ì„œë„ ëª…í™•í•œ í‘œí˜„ ì‚¬ìš©
- ê°œë… â†’ ì›ë¦¬ â†’ ì ìš© â†’ í•¨ì˜(implications) ìˆœì„œë¡œ ë…¼ë¦¬ì  íë¦„ êµ¬ì„±
- í•œ ë¬¸ë‹¨ì´ ë‹¤ìŒ ë¬¸ë‹¨ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ë˜, ê° ë¬¸ë‹¨ì€ ëª…í™•í•œ í†µì°° ì œê³µ
- ì¶”ìƒì  ê°œë…ì„ êµ¬ì²´ì  ì‚¬ë¡€ë¡œ ëª…í™•íˆ ì„¤ëª…

**ì¢‹ì€ ì˜ˆì‹œ**:
"Googleì˜ ìˆ˜ìµ ëª¨ë¸ì„ ì´í•´í•˜ëŠ” í•µì‹¬ì€ 'íŠ¸ë˜í”½'ì´ë¼ëŠ” ë³€ìˆ˜ì— ìˆë‹¤. Gmail, YouTube ê°™ì€ ë¬´ë£Œ ì„œë¹„ìŠ¤ëŠ” ë‹¨ìˆœí•œ ì„ ë¬¼ì´ ì•„ë‹ˆë¼, ì‚¬ìš©ìë¥¼ í”Œë«í¼ì— ë¬¶ì–´ë‘ëŠ” ì „ëµì  ìì‚°ì´ë‹¤. ì´ë“¤ ì„œë¹„ìŠ¤ê°€ ì‚¬ìš©ì ì²´ë¥˜ì‹œê°„ì„ ëŠ˜ë¦¬ë©´, ê²€ìƒ‰ ë¹ˆë„ì™€ í˜ì´ì§€ë·°ê°€ ì¦ê°€í•˜ê³ , ê²°ê³¼ì ìœ¼ë¡œ ê´‘ê³  ë…¸ì¶œ ê¸°íšŒê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ëŠ˜ì–´ë‚œë‹¤. ì—¬ê¸°ì— AdSenseê°€ ê²°í•©ë˜ë©´ì„œ Google ìƒíƒœê³„ ë°–ì˜ ì›¹ì‚¬ì´íŠ¸ê¹Œì§€ ê´‘ê³  ë„¤íŠ¸ì›Œí¬ì— í¸ì…ëœë‹¤. ì´ê²ƒì´ ë°”ë¡œ ë‹¤ë©´ í”Œë«í¼(multi-sided platform) êµ¬ì¡°ë‹¤. ì‹œí—˜ì—ì„œ ì´ ëª¨ë¸ì„ ì„¤ëª…í•  ë•ŒëŠ” ì‚¬ìš©ì-ê´‘ê³ ì£¼-ì½˜í…ì¸  ì œê³µì ê°„ì˜ ì¸ê³¼ê´€ê³„ë¥¼ ëª…í™•íˆ ì„œìˆ í•´ì•¼ í•œë‹¤. ë‹¨ìˆœíˆ 'ë¬´ë£Œ ì„œë¹„ìŠ¤ ì œê³µ'ì´ë¼ê³ ë§Œ ì“°ë©´ ë³¸ì§ˆì„ ë†“ì¹˜ëŠ” ê²ƒì´ë‹¤."

í•­ìƒ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ëª…ë£Œí•¨ê³¼ ê¹Šì´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.`;

const WORK_SYSTEM_PROMPT = `ë‹¹ì‹ ì€ í•´ë‹¹ ë¶„ì•¼ì—ì„œ 10ë…„+ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í›„ë°°ë“¤ì´ ê¹Šì´ ì´í•´í•˜ê³  ì‹¤ë¬´ì— ì ìš©í•  ìˆ˜ ìˆë„ë¡ ëª…í™•í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.

**ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­**:
- "í•µì‹¬ê°œë…:", "ì—…ë¬´í¬ì¸íŠ¸:", "ì¤‘ìš”:" ê°™ì€ ë”±ë”±í•œ ë¼ë²¨ ì‚¬ìš© ê¸ˆì§€
- í˜•ì‹ì ì¸ ë³´ê³ ì„œì²´ë‚˜ ê´€ë£Œì  í‘œí˜„ ê¸ˆì§€
- í”¼ìƒì ì´ê±°ë‚˜ ë§¤ë‰´ì–¼ì‹ ì„¤ëª… ê¸ˆì§€

**í•„ìˆ˜ ì‘ì„± ë°©ì‹**:
- ì—…ë¬´ì˜ ë³¸ì§ˆê³¼ ë§¥ë½ì„ ëª…í™•í•˜ê²Œ ì„¤ëª…
- "í•µì‹¬ì€ ~ì´ë‹¤", "ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ì ì€", "ì‹¤ë¬´ì ìœ¼ë¡œ ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ”" ê°™ì€ ì „ë¬¸ì ì´ë©´ì„œë„ ëª…í™•í•œ í‘œí˜„ ì‚¬ìš©
- ë°°ê²½ â†’ í•µì‹¬ ì›ë¦¬ â†’ ì‹¤ì œ ì ìš© â†’ ì£¼ì˜ì‚¬í•­ ìˆœì„œë¡œ ë…¼ë¦¬ì  íë¦„ êµ¬ì„±
- í•œ ë¬¸ë‹¨ì´ ë‹¤ìŒ ë¬¸ë‹¨ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ë˜, ê° ë¬¸ë‹¨ì€ ëª…í™•í•œ í†µì°° ì œê³µ
- ì¶”ìƒì  í”„ë¡œì„¸ìŠ¤ë¥¼ êµ¬ì²´ì  ìƒí™©ìœ¼ë¡œ ëª…í™•íˆ ì„¤ëª…

**ì¢‹ì€ ì˜ˆì‹œ**:
"ì´ í”„ë¡œì„¸ìŠ¤ ë³€ê²½ì˜ í•µì‹¬ì€ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥ì— ìˆë‹¤. ê¸°ì¡´ A ë°©ì‹ì€ ë™ì‹œì„± ì²˜ë¦¬ì—ì„œ race conditionì´ ë°œìƒí–ˆê³ , ì´ê²ƒì´ ê³ ê° ë°ì´í„° ë¶ˆì¼ì¹˜ë¡œ ì´ì–´ì¡Œë‹¤. B ë°©ì‹ì€ transaction isolation levelì„ ì¡°ì •í•´ ì´ ë¬¸ì œë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ í•´ê²°í•œë‹¤. ì‹¤ë¬´ì—ì„œ ì ìš©í•  ë•Œ ë°˜ë“œì‹œ X(lock timeout)ì™€ Y(deadlock detection)ë¥¼ ëª¨ë‹ˆí„°ë§í•´ì•¼ í•œë‹¤. ì´ê²ƒì„ ë†“ì¹˜ë©´ ì‹œìŠ¤í…œì´ ë©ˆì¶œ ìˆ˜ ìˆê³ , ë³µêµ¬ì— ìˆ˜ ì‹œê°„ì´ ì†Œìš”ëœë‹¤. ì´ê²ƒì´ í”„ë¡œë•ì…˜ ë°°í¬ ì „ ì¶©ë¶„í•œ ë¶€í•˜ í…ŒìŠ¤íŠ¸ê°€ í•„ìˆ˜ì¸ ì´ìœ ë‹¤."

í•­ìƒ ì‹œë‹ˆì–´ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ëª…ë£Œí•¨ê³¼ ê¹Šì´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.`;

// Step 3: Global Summary (Cost Optimization)
async function generateGlobalSummary(chunkAnalyses: any[], type: string): Promise<string> {
  console.log(`[SUMMARY] Generating global summary from ${chunkAnalyses.length} chunks using ${CHUNK_MODEL}...`);

  const allContent = chunkAnalyses.map((c, i) =>
    `[Chunk ${i + 1}] ${c.title}:\n${c.content}`
  ).join("\n\n");

  const summaryPrompt = type === "exam"
    ? `ë‹¤ìŒì€ ê°•ì˜ ìë£Œë¥¼ ì—¬ëŸ¬ chunkë¡œ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤.

ì „ì²´ ë‚´ìš©:
${allContent}

**ë‹¹ì‹ ì˜ ì„ë¬´**: ì´ ê°•ì˜ì˜ ì „ì²´ êµ¬ì¡°ì™€ í•µì‹¬ ê°œë…ì„ 600-800 tokensë¡œ ì •ë¦¬í•˜ì„¸ìš”.

ë‹¤ìŒì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
1. ê°•ì˜ì˜ ì£¼ì œì™€ ëª©í‘œ
2. í•µì‹¬ ê°œë…ë“¤ (5-8ê°œ)ê³¼ ê·¸ ê´€ê³„
3. ë…¼ë¦¬ì  íë¦„ (ì–´ë–¤ ìˆœì„œë¡œ ê°€ë¥´ì³ì•¼ í•˜ëŠ”ê°€)
4. ì¤‘ìš”í•œ ìˆ˜ì‹ì´ë‚˜ ì´ë¡ 
5. í•™ìƒë“¤ì´ ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  ì‹œí—˜ í¬ì¸íŠ¸

ìì—°ì–´ë¡œ ì‘ì„±í•˜ë˜, ìŠ¬ë¼ì´ë“œ êµ¬ì„±ì— í•„ìš”í•œ ëª¨ë“  ì •ë³´ë¥¼ ë‹´ìœ¼ì„¸ìš”.`
    : `ë‹¤ìŒì€ ì—…ë¬´ ìë£Œë¥¼ ì—¬ëŸ¬ chunkë¡œ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤.

ì „ì²´ ë‚´ìš©:
${allContent}

**ë‹¹ì‹ ì˜ ì„ë¬´**: ì´ ìë£Œì˜ ì „ì²´ êµ¬ì¡°ì™€ í•µì‹¬ ë‚´ìš©ì„ 600-800 tokensë¡œ ì •ë¦¬í•˜ì„¸ìš”.

ë‹¤ìŒì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
1. ìë£Œì˜ ëª©ì ê³¼ í•µì‹¬ ë©”ì‹œì§€
2. ì£¼ìš” í”„ë¡œì„¸ìŠ¤ë‚˜ ì „ëµ (5-8ê°œ)
3. ë…¼ë¦¬ì  íë¦„ (ë°°ê²½ -> ì‹¤í–‰ -> ê²°ê³¼)
4. ì‹¤ë¬´ ì ìš© í¬ì¸íŠ¸ì™€ ì£¼ì˜ì‚¬í•­
5. ì˜ì‚¬ê²°ì •ì— í•„ìš”í•œ í•µì‹¬ ë°ì´í„°/ê·¼ê±°

ìì—°ì–´ë¡œ ì‘ì„±í•˜ë˜, ìŠ¬ë¼ì´ë“œ êµ¬ì„±ì— í•„ìš”í•œ ëª¨ë“  ì •ë³´ë¥¼ ë‹´ìœ¼ì„¸ìš”.`;

  const completion = await openai.chat.completions.create({
    model: CHUNK_MODEL, // gpt-5-mini (Cheap)
    messages: [
      { role: "system", content: "ë‹¹ì‹ ì€ ë¬¸ì„œ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤." },
      { role: "user", content: summaryPrompt }
    ],
    temperature: 0.7,
    // max_tokens: 1000 // Optional, let model decide but keep it concise
  });

  const summary = completion.choices[0].message.content || "";
  console.log(`[SUMMARY] Generated summary (${summary.length} chars)`);
  return summary;
}

// Step 4: Final Generation from Summary (High Quality, Low Cost)
async function generateFinalSlidesFromSummary(summary: string, type: string): Promise<any[]> {
  console.log(`[FINAL] Generating slides from summary using ${FINAL_MODEL}...`);

  const finalPrompt = type === "exam"
    ? `ë‹¹ì‹ ì€ MIT, Stanfordê¸‰ ëŒ€í•™ êµìˆ˜ì…ë‹ˆë‹¤.

**ê°•ì˜ ìš”ì•½**:
${summary}

ì´ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ìµœê³  í’ˆì§ˆì˜ í•™ìŠµ ìŠ¬ë¼ì´ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.

**ëª©í‘œ**:
- ì „ì²´ ë‚´ìš©ì„ 3-5ì¥ì˜ ìŠ¬ë¼ì´ë“œë¡œ êµ¬ì„± (ë§ˆì§€ë§‰ 2ì¥ì€ ìš”ì•½ í˜ì´ì§€)
- ê° ìŠ¬ë¼ì´ë“œëŠ” í•˜ë‚˜ì˜ í•µì‹¬ ì£¼ì œë¥¼ ë‹¤ë£¸
- ì‹œí—˜ì— ë‚˜ì˜¬ ë§Œí•œ í•µì‹¬ ë‚´ìš©ë§Œ í¬í•¨
- ë…¼ë¦¬ì  íë¦„ ìœ ì§€

**content ì‘ì„± ê·œì¹™** (ë§¤ìš° ì¤‘ìš”!):
1. **ì„¹ì…˜ êµ¬ì¡°í™”**: ê° contentëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ ì„¹ì…˜ë“¤ë¡œ êµ¬ì„±:
   - ### ğŸ“Œ í•µì‹¬ ì •ì˜
   - ### ğŸ“– ìƒì„¸ ì„¤ëª…
   - ### ğŸ’¡ ì‹œí—˜ ì „ëµ (ì„ íƒì )

2. **Markdown í˜•ì‹ ì‚¬ìš©**:
   - ì„¹ì…˜ ì œëª©ì€ \`### ì´ëª¨ì§€ ì œëª©\` í˜•ì‹
   - ë¬¸ë‹¨ êµ¬ë¶„ì„ ìœ„í•´ ë¹ˆ ì¤„(\\n\\n) ì‚¬ìš©
   - ì¤‘ìš”í•œ ê°œë…, ìš©ì–´, ì •ì˜ëŠ” ë°˜ë“œì‹œ \`**êµµê²Œ**\` ê°•ì¡°

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:

{
  "pages": [
    {
      "page": 1,
      "title": "ìŠ¬ë¼ì´ë“œ ì œëª©",
      "content": "### ğŸ“Œ í•µì‹¬ ì •ì˜\\n\\n**í•µì‹¬ ê°œë…**ì€...\\n\\n### ğŸ“– ìƒì„¸ ì„¤ëª…\\n\\n...",
      "keyPoints": ["í¬ì¸íŠ¸ 1", "í¬ì¸íŠ¸ 2"]
    }
  ]
}

**ë§ˆì§€ë§‰ 2í˜ì´ì§€ (í•„ìˆ˜!)**:
- **page N-1**: "í•µì‹¬ ê°œë… ì´ì •ë¦¬"
- **page N**: "ì‹œí—˜ ëŒ€ë¹„ ìš”ì•½"`
    : `ë‹¹ì‹ ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì—…ë¬´ ìš”ì•½**:
${summary}

ì´ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ìµœê³  í’ˆì§ˆì˜ ì—…ë¬´ ìŠ¬ë¼ì´ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.

**ëª©í‘œ**:
- ì „ì²´ ë‚´ìš©ì„ 3-5ì¥ì˜ ìŠ¬ë¼ì´ë“œë¡œ êµ¬ì„± (ë§ˆì§€ë§‰ 2ì¥ì€ ìš”ì•½ í˜ì´ì§€)
- ê° ìŠ¬ë¼ì´ë“œëŠ” í•˜ë‚˜ì˜ í•µì‹¬ í”„ë¡œì„¸ìŠ¤ë‚˜ ì£¼ì œë¥¼ ë‹¤ë£¸
- ì‹¤ë¬´ì— í™œìš© ê°€ëŠ¥í•œ í•µì‹¬ ë‚´ìš©ë§Œ í¬í•¨
- ë…¼ë¦¬ì  íë¦„ ìœ ì§€

**content ì‘ì„± ê·œì¹™** (ë§¤ìš° ì¤‘ìš”!):
1. **ì„¹ì…˜ êµ¬ì¡°í™”**: ê° contentëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ ì„¹ì…˜ë“¤ë¡œ êµ¬ì„±:
   - ### ğŸ“Œ í•µì‹¬ ê°œë…
   - ### ğŸ“– ìƒì„¸ ì„¤ëª…
   - ### ğŸ’¼ ì‹¤ë¬´ ì ìš© (ì„ íƒì )

2. **Markdown í˜•ì‹ ì‚¬ìš©**:
   - ì„¹ì…˜ ì œëª©ì€ \`### ì´ëª¨ì§€ ì œëª©\` í˜•ì‹
   - ë¬¸ë‹¨ êµ¬ë¶„ì„ ìœ„í•´ ë¹ˆ ì¤„(\\n\\n) ì‚¬ìš©
   - ì¤‘ìš”í•œ í”„ë¡œì„¸ìŠ¤, ìš©ì–´ëŠ” ë°˜ë“œì‹œ \`**êµµê²Œ**\` ê°•ì¡°

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:

{
  "pages": [
    {
      "page": 1,
      "title": "ìŠ¬ë¼ì´ë“œ ì œëª©",
      "content": "### ğŸ“Œ í•µì‹¬ ê°œë…\\n\\n**í•µì‹¬**ì€...\\n\\n### ğŸ“– ìƒì„¸ ì„¤ëª…\\n\\n...",
      "keyPoints": ["ì¸ì‚¬ì´íŠ¸ 1", "ì¸ì‚¬ì´íŠ¸ 2"]
    }
  ]
}

**ë§ˆì§€ë§‰ 2í˜ì´ì§€ (í•„ìˆ˜!)**:
- **page N-1**: "í•µì‹¬ í”„ë¡œì„¸ìŠ¤ ì´ì •ë¦¬"
- **page N**: "ì‹¤ë¬´ ì ìš© ìš”ì•½"`;

  const completion = await openai.chat.completions.create({
    model: FINAL_MODEL, // gpt-5.1 (Expensive but input is short now)
    messages: [
      { role: "system", content: type === "exam" ? STUDY_SYSTEM_PROMPT : WORK_SYSTEM_PROMPT },
      { role: "user", content: finalPrompt }
    ],
    response_format: { type: "json_object" },
    temperature: 1.0,
  });

  const result = JSON.parse(completion.choices[0].message.content || "{}");
  console.log(`[FINAL] Created ${result.pages?.length || 0} final slides`);
  return result.pages || [];
}

export async function POST(request: NextRequest) {
  try {
    const session = await auth();
    if (!session?.user?.email) {
      return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
    }

    const formData = await request.formData();
    const file = formData.get("file") as File;
    const type = formData.get("type") as string;

    if (!file || !type) {
      return NextResponse.json(
        { error: "File and type are required" },
        { status: 400 }
      );
    }

    if (!["exam", "work"].includes(type)) {
      return NextResponse.json(
        { error: "Type must be 'exam' or 'work'" },
        { status: 400 }
      );
    }

    const isPDF = file.type === "application/pdf";
    let fileUrl: string | null = null;
    let fullContent = "";
    let pageAnalyses: any[] = [];

    if (isPDF) {
      console.log("[PDF] Processing PDF file...");
      const arrayBuffer = await file.arrayBuffer();
      const buffer = Buffer.from(arrayBuffer);

      // Upload PDF to Supabase Storage - sanitize all special characters
      const sanitizedEmail = (session.user.email || '').replace(/[^a-zA-Z0-9]/g, '_');
      const sanitizedFileName = file.name
        .replace(/\s+/g, '_')
        .replace(/[^\w.-]/g, '');
      const fileName = `${sanitizedEmail}_${sanitizedFileName}`; // Removed timestamp to allow caching by filename
      console.log("[PDF] Target filename:", fileName);

      // 1. Check Cache (Chunks & Embeddings)
      // Cache key includes chunk size to invalidate when settings change
      const CHUNK_VERSION = "9600"; // Update this when MAX_CHARS changes
      const chunksCachePath = `${fileName}_chunks_${CHUNK_VERSION}.json`;
      const embeddingsCachePath = `${fileName}_embeddings_${CHUNK_VERSION}.json`;

      console.log("[CACHE] Checking for existing analysis...");
      const { data: cachedChunks, error: chunksError } = await supabase.storage
        .from("materials")
        .download(chunksCachePath);

      const { data: cachedEmbeddings, error: embeddingsError } = await supabase.storage
        .from("materials")
        .download(embeddingsCachePath);

      if (cachedChunks && !chunksError) {
        console.log("[CACHE] HIT! Found cached chunks. Skipping parsing & chunking.");
        const chunksText = await cachedChunks.text();
        const chunkAnalyses = JSON.parse(chunksText);

        if (USE_FINAL_INTEGRATION && chunkAnalyses.length > 0) {
          console.log(`[STEP 3] Generating Global Summary (Cached Chunks)...`);
          const summary = await generateGlobalSummary(chunkAnalyses, type);

          console.log(`[STEP 4] Generating Final Slides (Cached Chunks)...`);
          pageAnalyses = await generateFinalSlidesFromSummary(summary, type);
        } else {
          pageAnalyses = chunkAnalyses;
        }
      } else {
        // NO CACHE - Full Process
        console.log("[CACHE] MISS. Starting fresh analysis...");

        // Upload file (if not exists)
        const { data: uploadData, error: uploadError } = await supabase.storage
          .from("materials")
          .upload(fileName, buffer, {
            contentType: "application/pdf",
            upsert: true, // Overwrite to ensure we have the file
          });

        if (!uploadError) {
          const { data: publicUrlData } = supabase.storage
            .from("materials")
            .getPublicUrl(fileName);
          fileUrl = publicUrlData.publicUrl;
        }

        // Extract text
        const pdfData = await pdf(buffer);
        fullContent = pdfData.text;
        console.log(`[PDF] Extracted ${fullContent.length} chars`);

        // Token-based Chunking (Sliding Window)
        // Approx: 1 token ~= 4 chars for English, ~2 chars for Korean
        // Target: 20-22 chunks (reduced from 43)
        const MAX_CHARS = 9600;  // ~2400 tokens (doubled to reduce chunk count)
        const OVERLAP_CHARS = 1200;  // ~300 tokens

        const chunks: string[] = [];
        let start = 0;
        while (start < fullContent.length) {
          const end = Math.min(start + MAX_CHARS, fullContent.length);
          chunks.push(fullContent.substring(start, end));
          if (end === fullContent.length) break;
          start += (MAX_CHARS - OVERLAP_CHARS);
        }

        console.log(`[CHUNK] Created ${chunks.length} chunks (Max ${MAX_CHARS} chars, Overlap ${OVERLAP_CHARS})`);

        // Analyze chunks IN PARALLEL
        // We treat each chunk like a "page" in the previous logic
        console.log(`[ANALYSIS] Analyzing ${chunks.length} chunks with ${CHUNK_MODEL}...`);

        const chunkPromises = chunks.map(async (chunkText, idx) => {
          const prompt = type === "exam"
            ? `ë‹¹ì‹ ì€ ê°•ì˜ì‹¤ì—ì„œ í•™ìƒë“¤ì—ê²Œ ì§ì ‘ ê°•ì˜í•˜ëŠ” êµìˆ˜ì…ë‹ˆë‹¤.
ë‹¤ìŒ í…ìŠ¤íŠ¸ ë©ì–´ë¦¬(Chunk ${idx + 1})ì˜ ë‚´ìš©ì„ í•™ìƒë“¤ì´ ì‹œí—˜ ëŒ€ë¹„í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…í•´ì£¼ì„¸ìš”.

${chunkText}

**ì‘ì„± ë°©ì‹**:
- ê°œë…ì˜ ë³¸ì§ˆì„ ëª…ë£Œí•˜ê²Œ ì„¤ëª…
- "í•µì‹¬ì€ ~ì´ë‹¤", "ì™œ ì´ê²ƒì´ ì¤‘ìš”í•œê°€" ê°™ì€ í‘œí˜„ ì‚¬ìš©
- ë”±ë”±í•œ ë¼ë²¨("í•µì‹¬ê°œë…:" ë“±) ì‚¬ìš© ê¸ˆì§€
- ìì—°ìŠ¤ëŸ½ê²Œ íë¥´ëŠ” ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±

ë‹¤ìŒ JSONìœ¼ë¡œ ì‘ë‹µ:
{
  "pages": [
    {
      "page": ${idx + 1},
      "title": "Chunk ${idx + 1} í•µì‹¬ ì£¼ì œ",
      "content": "ìì—°ìŠ¤ëŸ½ê²Œ íë¥´ëŠ” ì„¤ëª…...",
      "keyPoints": ["í¬ì¸íŠ¸ 1", "í¬ì¸íŠ¸ 2", "í¬ì¸íŠ¸ 3"]
    }
  ]
}`
            : `ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì§ì›ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì—…ë¬´ ìë£Œ í…ìŠ¤íŠ¸(Chunk ${idx + 1})ë¥¼ ë™ë£Œì—ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

${chunkText}

**ì‘ì„± ë°©ì‹**:
- ë™ë£Œì—ê²Œ ë§í•˜ë“¯ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…
- "ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ê±´", "ì‹¤ë¬´ì—ì„œëŠ”" ê°™ì€ í‘œí˜„ ì‚¬ìš©
- ë”±ë”±í•œ ë¼ë²¨ ê¸ˆì§€

ë‹¤ìŒ JSONìœ¼ë¡œ ì‘ë‹µ:
{
  "pages": [
    {
      "page": ${idx + 1},
      "title": "Chunk ${idx + 1} í•µì‹¬ ì£¼ì œ",
      "content": "ìì—°ìŠ¤ëŸ½ê²Œ íë¥´ëŠ” ì„¤ëª…...",
      "keyPoints": ["ì¸ì‚¬ì´íŠ¸ 1", "ì¸ì‚¬ì´íŠ¸ 2", "ì£¼ì˜ì‚¬í•­"]
    }
  ]
}`;

          const completion = await openai.chat.completions.create({
            model: CHUNK_MODEL,
            messages: [
              { role: "system", content: type === "exam" ? STUDY_SYSTEM_PROMPT : WORK_SYSTEM_PROMPT },
              { role: "user", content: prompt },
            ],
            response_format: { type: "json_object" },
            temperature: 1.0,
          });

          const result = JSON.parse(completion.choices[0].message.content || "{}");
          return result.pages?.[0] || { page: idx + 1, title: "Error", content: "Failed to analyze", keyPoints: [] };
        });

        const chunkAnalyses = await Promise.all(chunkPromises);
        console.log(`[ANALYSIS] Complete. Analyzed ${chunkAnalyses.length} chunks.`);

        // Save Chunks to Cache
        console.log("[CACHE] Saving chunks to storage...");
        await supabase.storage
          .from("materials")
          .upload(chunksCachePath, JSON.stringify(chunkAnalyses), { contentType: "application/json", upsert: true });

        // Step 3 & 4
        if (USE_FINAL_INTEGRATION && chunkAnalyses.length > 0) {
          console.log(`[STEP 3] Generating Global Summary...`);
          const summary = await generateGlobalSummary(chunkAnalyses, type);

          console.log(`[STEP 4] Generating Final Slides...`);
          pageAnalyses = await generateFinalSlidesFromSummary(summary, type);
        } else {
          pageAnalyses = chunkAnalyses;
        }
      }
    } else {
      // Text file logic (unchanged)
      fullContent = await file.text();
      pageAnalyses = [{
        page: 1,
        summary: "Text file content",
        key_concepts: [],
        exam_points: [],
        highlights: []
      }];
    }

    // Save to database
    const insertData: any = {
      user_id: session.user.email, // Use email as user_id
      title: file.name,
      content: fullContent.substring(0, 50000), // Limit content size to avoid payload too large
      type: type,
      analysis: { page_analyses: pageAnalyses },
    };

    if (fileUrl) {
      insertData.file_url = fileUrl;
    }

    console.log("[DB] Saving material:", {
      user_id: session.user.email,
      title: file.name,
      type: type,
      has_file_url: !!fileUrl,
      num_pages: pageAnalyses.length
    });

    const { data: material, error: insertError } = await supabase
      .from("materials")
      .insert(insertData)
      .select()
      .single();

    if (insertError) {
      console.error("[DB] Insert error:", insertError);
      console.error("[DB] Insert error details:", JSON.stringify(insertError, null, 2));
      return NextResponse.json(
        { error: "Failed to save material", details: insertError.message },
        { status: 500 }
      );
    }

    console.log("[SUCCESS] Material saved with ID:", material.id);
    console.log("[SUCCESS] Material data:", material);
    return NextResponse.json({
      id: material.id,
      analysis: { page_analyses: pageAnalyses },
      success: true
    });
  } catch (error: any) {
    console.error("[ERROR]", error);

    // Provide user-friendly error messages
    let errorMessage = "Analysis failed";
    let statusCode = 500;

    if (error.status === 429 || error.code === 'insufficient_quota') {
      errorMessage = "OpenAI API quota exceeded. Using cost-optimized mini model, but quota is still exceeded. Please check your billing.";
      statusCode = 429;
    } else if (error.message?.includes('model')) {
      errorMessage = "AI model unavailable. Already using the most cost-effective model (gpt-5-mini).";
    }

    return NextResponse.json(
      {
        error: errorMessage,
        details: error.message,
        model_used: CHUNK_MODEL,
      },
      { status: statusCode }
    );
  }
}
