import { NextRequest } from "next/server";
import { auth } from "@/auth";
import { createClient } from "@supabase/supabase-js";
import OpenAI from "openai";
import pdfParse from "pdf-parse-fork";
import crypto from "crypto";
import { extractAndAnalyzeImages, formatImageAnalysis } from "@/lib/pdf-image-extractor";

// Route segment config for large file uploads
export const maxDuration = 300; // 5 minutes for Vercel Pro
export const dynamic = 'force-dynamic';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
});

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!,
  {
    auth: {
      autoRefreshToken: false,
      persistSession: false,
    },
  }
);

// Model Configuration
const MINI_MODEL = "gpt-5-mini-2025-08-07";      // Intelligent chunking
const ADVANCED_MODEL = "gpt-5.1-2025-11-13";    // High-quality conversion

/**
 * SMART HYBRID PIPELINE (2025-12-04)
 *
 * ëª©ì : í’ˆì§ˆ ìœ ì§€ + ë¹„ìš© ì ˆê°
 *
 * Architecture:
 * 1. PDF â†’ Text Extraction (pdf-parse-fork)
 * 2. GPT-5 Mini: ì›ë¬¸ì„ 3-4ê°œ ê· ë“± ì²­í¬ë¡œ ë¶„í•  (ì»¨í…ìŠ¤íŠ¸ ìœ ì§€)
 * 3. GPT-5.1 ìˆœì°¨: ê° ì²­í¬ë¥¼ A ëª¨ë“œë¡œ ë³€í™˜ (ì´ì „ ê²°ê³¼ ì°¸ì¡°)
 *
 * ë¹„ìš© ìµœì í™”:
 * - Mini 1íšŒ: ~$0.001 (ì²­í¬ ë¶„í• )
 * - Advanced 3-4íšŒ: ~$0.03-0.05 (ê° ì²­í¬ 2-3K output)
 * - ì´: ~$0.04-0.06 (ê¸°ì¡´ $0.15 ëŒ€ë¹„ 60-73% ì ˆê°)
 */

export async function POST(request: NextRequest) {
  const encoder = new TextEncoder();
  const stream = new TransformStream();
  const writer = stream.writable.getWriter();

  const sendEvent = async (event: string, data: any) => {
    await writer.write(encoder.encode(`event: ${event}\ndata: ${JSON.stringify(data)}\n\n`));
  };

  (async () => {
    const startTime = Date.now();

    try {
      const session = await auth();
      if (!session?.user?.email) {
        await sendEvent("error", { error: "Unauthorized" });
        await writer.close();
        return;
      }

      const body = await request.json();
      const { blobUrl, fileName: originalFileName, type } = body;

      if (!blobUrl || !originalFileName) {
        await sendEvent("error", { error: "No file URL provided" });
        await writer.close();
        return;
      }

      console.log(`[START] Processing ${originalFileName} from blob: ${blobUrl}`);

      // ====================
      // STEP 1: Download from Blob & Extract Text
      // ====================
      await sendEvent("progress", {
        stage: "download_and_extract",
        message: "íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."
      });

      // Download file from blob storage
      const blobResponse = await fetch(blobUrl);
      if (!blobResponse.ok) {
        throw new Error("Failed to download file from blob storage");
      }

      const buffer = Buffer.from(await blobResponse.arrayBuffer());
      const fileHash = crypto.createHash('sha256').update(buffer).digest('hex');
      const fileName = `${session.user.email}/${fileHash}.pdf`;

      // Upload to Supabase
      let fileUrl = "";
      const { error: uploadError } = await supabase.storage
        .from("materials")
        .upload(fileName, buffer, {
          contentType: "application/pdf",
          upsert: true,
        });

      if (!uploadError) {
        const { data: publicUrlData } = supabase.storage.from("materials").getPublicUrl(fileName);
        fileUrl = publicUrlData.publicUrl;
        console.log(`[STORAGE] Uploaded PDF: ${fileUrl}`);
      } else {
        console.error("[STORAGE] Upload error:", uploadError);
        throw new Error("Failed to upload PDF");
      }

      // Extract text from PDF
      console.log('[PDF-EXTRACT] Extracting text...');
      const pdfData = await pdfParse(buffer);
      let extractedText = pdfData.text;
      const pageCount = pdfData.numpages;

      console.log(`[PDF-PARSE] Extracted ${extractedText.length} chars from ${pageCount} pages`);

      // Extract and analyze images with Google Cloud Vision
      await sendEvent("progress", {
        stage: "image_analysis",
        message: "ì´ë¯¸ì§€ ë¶„ì„ ì¤‘... (Google Cloud Vision)"
      });

      console.log('[IMAGE-EXTRACT] Analyzing images...');
      const apiKey = process.env.GEMINI_API_KEY;
      if (apiKey) {
        const imageAnalyses = await extractAndAnalyzeImages(buffer, apiKey);
        if (imageAnalyses.length > 0) {
          const imageText = formatImageAnalysis(imageAnalyses);
          extractedText = extractedText + imageText;
          console.log(`[IMAGE-EXTRACT] Added ${imageAnalyses.length} image analyses to text`);
        } else {
          console.log('[IMAGE-EXTRACT] No images found or analyzed');
        }
      } else {
        console.warn('[IMAGE-EXTRACT] Skipping - no API key found');
      }

      // ====================
      // STEP 2: GPT-5 Mini - ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í• 
      // ====================
      await sendEvent("progress", {
        stage: "chunking",
        message: "ìµœì  ì²­í¬ë¡œ ë¶„í•  ì¤‘... (GPT-5 Mini)"
      });

      console.log('[GPT-5 Mini] Smart chunking...');

      // Simple character-based chunking (ê· ë“± ë¶„í• )
      const targetChunkSize = Math.ceil(extractedText.length / 3); // 3ë“±ë¶„
      const chunks: { index: number; text: string }[] = [];

      let currentPos = 0;
      let chunkIndex = 0;

      while (currentPos < extractedText.length) {
        const endPos = Math.min(currentPos + targetChunkSize, extractedText.length);
        let actualEndPos = endPos;

        // Try to break at paragraph boundary
        if (actualEndPos < extractedText.length) {
          const nextNewline = extractedText.indexOf('\n\n', endPos - 200);
          if (nextNewline !== -1 && nextNewline < endPos + 200) {
            actualEndPos = nextNewline + 2;
          }
        }

        chunks.push({
          index: chunkIndex,
          text: extractedText.substring(currentPos, actualEndPos).trim()
        });

        currentPos = actualEndPos;
        chunkIndex++;
      }

      console.log(`[CHUNKING] Split into ${chunks.length} chunks (avg ${Math.round(extractedText.length / chunks.length)} chars each)`);

      // ====================
      // STEP 3: Save Material to DB
      // ====================
      console.log('[DB] Creating material record...');

      const material = {
        user_id: session.user.email,
        title: originalFileName.replace(/\.(pdf|txt|doc|docx)$/i, ''),
        content: extractedText.substring(0, 50000),
        type,
        file_url: fileUrl,
        file_hash: fileHash,
        analysis: {
          status: 'processing',
          approach: "smart-hybrid"
        },
        created_at: new Date().toISOString(),
      };

      const { data: insertedMaterial, error: insertError } = await supabase
        .from("materials")
        .insert(material)
        .select()
        .single();

      if (insertError) {
        console.error("[DB] Insert error:", insertError);
        throw insertError;
      }

      console.log(`[DB] Material created with ID: ${insertedMaterial.id}`);

      // ====================
      // STEP 4: GPT-5.1 ìˆœì°¨ ë³€í™˜ (ì»¨í…ìŠ¤íŠ¸ ìœ ì§€)
      // ====================
      await sendEvent("progress", {
        stage: "converting",
        message: "A ëª¨ë“œë¡œ ë³€í™˜ ì¤‘... (GPT-5.1)"
      });

      console.log(`[GPT-5.1 + Mini] Converting and enhancing ${chunks.length} chunks in parallel...`);

      // Parallel conversion AND enhancement with Promise.all
      const conversionPromises = chunks.map(async (chunk, i) => {
        // ìœ ë‹ˆì½”ë“œ ì•„ë˜ì²¨ìë¥¼ ì¼ë°˜ ë¬¸ìë¡œ ì •ê·œí™”
        const normalizeSubscripts = (text: string): string => {
          // ìœ ë‹ˆì½”ë“œ ì•„ë˜ì²¨ì ìˆ«ì â†’ ì¼ë°˜ ìˆ«ì
          const subscriptDigits: Record<string, string> = {
            'â‚€': '0', 'â‚': '1', 'â‚‚': '2', 'â‚ƒ': '3', 'â‚„': '4',
            'â‚…': '5', 'â‚†': '6', 'â‚‡': '7', 'â‚ˆ': '8', 'â‚‰': '9'
          };

          // ìœ ë‹ˆì½”ë“œ ì•„ë˜ì²¨ì ë¬¸ì â†’ ì¼ë°˜ ë¬¸ì
          const subscriptLetters: Record<string, string> = {
            'â‚': 'a', 'â‚‘': 'e', 'â‚’': 'o', 'â‚“': 'x', 'â‚•': 'h',
            'â‚–': 'k', 'â‚—': 'l', 'â‚˜': 'm', 'â‚™': 'n', 'â‚š': 'p',
            'â‚›': 's', 'â‚œ': 't', 'áµ¢': 'i', 'áµ£': 'r', 'áµ¤': 'u',
            'áµ¥': 'v', 'â±¼': 'j'
          };

          let normalized = text;

          // ìˆ«ì ì•„ë˜ì²¨ì ì¹˜í™˜
          Object.entries(subscriptDigits).forEach(([unicode, normal]) => {
            normalized = normalized.replace(new RegExp(unicode, 'g'), normal);
          });

          // ë¬¸ì ì•„ë˜ì²¨ì ì¹˜í™˜
          Object.entries(subscriptLetters).forEach(([unicode, normal]) => {
            normalized = normalized.replace(new RegExp(unicode, 'g'), normal);
          });

          return normalized;
        };

        const normalizedText = normalizeSubscripts(chunk.text);

        const conversionPrompt = `ë‹¹ì‹ ì€ **ë³€í™˜ê¸°**ì…ë‹ˆë‹¤. ì•„ë˜ ì›ë¬¸ì„ A ëª¨ë“œë¡œ ì••ì¶• ë³€í™˜í•˜ì„¸ìš”.

**í˜„ì¬ ì›ë¬¸** (ì „ì²´ ì¤‘ ${i + 1}/${chunks.length} ë¶€ë¶„):
"""
${normalizedText}
"""

**ë³€í™˜ ê·œì¹™ (A ëª¨ë“œ)**:

1. **ì›ë¬¸ ë¬¸ì¥ ê¸°ì¤€**: ì›ë¬¸ì˜ ë¬¸ì¥ êµ¬ì¡°ì™€ ìˆœì„œ ìœ ì§€
2. **ë¬¸ë‹¨ ë‹¨ìœ„ ì²˜ë¦¬**: ì›ë¬¸ì˜ ë¬¸ë‹¨ ìˆœì„œ ê·¸ëŒ€ë¡œ ìœ ì§€
3. **ì¬í•´ì„Â·ì°½ì‘ ê¸ˆì§€**: ì›ë¬¸ì— ì—†ëŠ” ë‚´ìš© ì ˆëŒ€ ì¶”ê°€ ë¶ˆê°€
4. **í˜•ì‹ ë³€í™˜ë§Œ**:
   - í•µì‹¬ ìš©ì–´: **êµµê²Œ**
   - ê°€ì¥ ì¤‘ìš”í•œ ìš©ì–´: <mark>í•˜ì´ë¼ì´íŠ¸</mark> (ë°˜ë“œì‹œ ì—¬ëŠ” íƒœê·¸ì™€ ë‹«ëŠ” íƒœê·¸ ëª¨ë‘ í¬í•¨)
   - ìˆ˜ì‹: ë°˜ë“œì‹œ LaTeX í˜•ì‹ ($x^2$ ì¸ë¼ì¸ ë˜ëŠ” $$E=mc^2$$ ë¸”ë¡)
   - í‘œ: ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ë³€í™˜
5. **ì–¸ì–´**:
   - ì„¤ëª…ê³¼ ë¬¸ì¥ì€ í•œêµ­ì–´ë¡œ ì‘ì„±
   - ì „ë¬¸ ìš©ì–´, ê³ ìœ ëª…ì‚¬, ê¸°ìˆ  ìš©ì–´ëŠ” ì˜ì–´ ê·¸ëŒ€ë¡œ ìœ ì§€
   - ì˜ˆ: "**Gradient Descent**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤"
6. **ë¶ˆí™•ì‹¤í•˜ë©´ ì›ë¬¸ ì¸ìš©**: í•´ì„í•˜ì§€ ë§ê³  ì›ë¬¸ ê·¸ëŒ€ë¡œ ë³µì‚¬
7. **ì••ì¶• ê°•ë„ (ë§¤ìš° ì¤‘ìš”)**:
   - ëª©í‘œ: ì›ë³¸ ëŒ€ë¹„ 60-65% ë¶„ëŸ‰ìœ¼ë¡œ ì••ì¶•
   - ë°˜ë³µ ì„¤ëª…, ì¥í™©í•œ ì˜ˆì‹œ, ë¶ˆí•„ìš”í•œ ì ‘ì†ì‚¬ ì œê±°
   - í•µì‹¬ë§Œ ë‚¨ê¸°ë˜ ì¤‘ìš” ë‚´ìš©ì€ ì ˆëŒ€ ëˆ„ë½ ê¸ˆì§€
   - ê°™ì€ ë§¥ë½ì˜ ì—¬ëŸ¬ ë¬¸ì¥ì€ í•˜ë‚˜ë¡œ í†µí•©

**ìŠ¬ë¼ì´ë“œ êµ¬ë¶„ (ë§¤ìš° ì¤‘ìš”)**:
- **ìŠ¬ë¼ì´ë“œ ê°œìˆ˜**: ë‚´ìš©ì— ë”°ë¼ ìì—°ìŠ¤ëŸ½ê²Œ ì¡°ì • (ì œí•œ ì—†ìŒ)
- í° ì£¼ì œ/ì„¹ì…˜/ê°œë…ì€ \`## ì œëª©\` í˜•ì‹ìœ¼ë¡œ ì‹œì‘í•˜ì—¬ ê°ê° ë³„ë„ ìŠ¬ë¼ì´ë“œë¡œ êµ¬ì„±
- **ë‚´ìš©ì´ ë§ì€ ê²½ìš°**: ì£¼ì œë³„ë¡œ ì ì ˆíˆ ë¶„í• í•˜ì—¬ ê°ê° ìŠ¬ë¼ì´ë“œë¡œ ìƒì„±
- **ë‚´ìš©ì´ ì ì€ ê²½ìš°**: ê´€ë ¨ ê°œë…ì„ í†µí•©í•˜ì—¬ ì ì ˆí•œ ê°œìˆ˜ë¡œ ì¡°ì •
- ê° ìŠ¬ë¼ì´ë“œëŠ” í•˜ë‚˜ì˜ ì™„ê²°ëœ ì£¼ì œë‚˜ ê°œë…ì„ ë‹´ì„ ê²ƒ
- ì‘ì€ ì„¸ë¶€ ì£¼ì œë“¤ì€ ### ì†Œì œëª©ìœ¼ë¡œ í•˜ë‚˜ì˜ ìŠ¬ë¼ì´ë“œ ì•ˆì—ì„œ êµ¬ë¶„
- **ì ˆëŒ€ ê¸ˆì§€**: ë¹ˆ ìŠ¬ë¼ì´ë“œë‚˜ ì˜ë¯¸ì—†ëŠ” ìŠ¬ë¼ì´ë“œë¥¼ ë§Œë“¤ì§€ ë§ ê²ƒ
- **ì¤‘ìš”**: ì›ë³¸ì— ì‹¤ì œë¡œ ìˆëŠ” ë‚´ìš©ë§Œ ìŠ¬ë¼ì´ë“œë¡œ ë§Œë“¤ ê²ƒ

**ì œëª© ì‘ì„± ê·œì¹™ (ë§¤ìš° ë§¤ìš° ì¤‘ìš” - ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ!)**:
- **ê¹”ë”í•œ ì œëª©ë§Œ ì‚¬ìš©**: "Futures Markets (Introduction)" âœ…
- **ì ˆëŒ€ ê¸ˆì§€ - ë‹¤ìŒì€ ëª¨ë‘ ì œê±°**:
  - âŒ "Lecture 12: Futures Markets" â†’ "Futures Markets" âœ…
  - âŒ "Chapter 3: Introduction" â†’ "Introduction" âœ…
  - âŒ "Section 2.1: Basic Concepts" â†’ "Basic Concepts" âœ…
  - âŒ "1. Derivatives Overview" â†’ "Derivatives Overview" âœ…
  - âŒ "<mark>Futures</mark> Markets" â†’ "Futures Markets" âœ…
  - âŒ **HTML íƒœê·¸ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€** (<mark>, <strong>, <em> ë“±)
  - âŒ "Lecture N:", "Chapter N:", "Section N:" ê°™ì€ ì ‘ë‘ì–´
  - âŒ ìˆ«ìë¡œ ì‹œì‘ (1., 2.1, 3-2 ë“±)
- **ì˜¬ë°”ë¥¸ ì˜ˆì‹œ**:
  - âœ… "Futures Markets"
  - âœ… "Spot-Futures Parity"
  - âœ… "Cost of Carry Model"
  - âœ… "Hedging Strategies"

**ìˆ˜ì‹ ë³€í™˜ (ë§¤ìš° ë§¤ìš° ì¤‘ìš” - ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ!)**:

**í•µì‹¬ ì›ì¹™**: ì›ë¬¸ì˜ ëª¨ë“  ìˆ˜ì‹, ê³µì‹, ìˆ˜í•™ì  í‘œí˜„ì€ **100% LaTeX í˜•ì‹**ìœ¼ë¡œ ë³€í™˜!

**LaTeX ë¬¸ë²• ì˜ˆì‹œ**:
1. **ì¸ë¼ì¸ ìˆ˜ì‹** (ë¬¸ì¥ ì•ˆ): $F_0 = S_0(1 + r_f - d)^T$
2. **ë¸”ë¡ ìˆ˜ì‹** (ë…ë¦½ í–‰): $$F_0 = S_0(1 + r_f - d)^T$$
3. **ë¶„ìˆ˜**: $\\frac{a}{b}$, $\\frac{numerator}{denominator}$
4. **ê±°ë“­ì œê³±**: $x^2$, $e^{-x}$, $(1+r)^T$
5. **ì•„ë˜ì²¨ì (ë§¤ìš° ì¤‘ìš”!)**:
   - ìˆ«ì ì•„ë˜ì²¨ì: $F_0$, $S_0$, $x_1$, $x_2$ (underscore ë’¤ì— ìˆ«ì)
   - ë¬¸ì ì•„ë˜ì²¨ì: $r_f$, $r_d$, $x_i$, $x_n$ (underscore ë’¤ì— ì˜ë¬¸ì)
   - **ì ˆëŒ€ ê¸ˆì§€**: $F_!$, $S_!$, $r_"#$ (íŠ¹ìˆ˜ë¬¸ì ì‚¬ìš© ê¸ˆì§€!)
6. **ìˆ«ì í‘œê¸° (ë§¤ìš° ì¤‘ìš”!)**:
   - ì²œ ë‹¨ìœ„ êµ¬ë¶„ ì‰¼í‘œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€: $1,894$, $3.00$
   - **ì ˆëŒ€ ê¸ˆì§€**: $1{,}894$ (ì˜ëª»ëœ LaTeX ë¬¸ë²•)
7. **ê·¸ë¦¬ìŠ¤ ë¬¸ì**: $\\alpha$, $\\beta$, $\\mu$, $\\sigma$, $\\pi$
8. **í•©/ì ë¶„**: $\\sum_{i=1}^n x_i$, $\\int_a^b f(x)dx$
9. **í–‰ë ¬**: $$\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$$
10. **ì œê³±ê·¼**: $\\sqrt{x}$, $\\sqrt[n]{x}$
11. **ê·¹í•œ**: $\\lim_{x \\to \\infty}$

**ë³€í™˜ ì˜ˆì‹œ**:

**ì›ë¬¸**: "The spot-futures parity formula is F0 = S0(1 + rf - d)^T"
**ì˜¬ë°”ë¥¸ ë³€í™˜**: "spot-futures parity ê³µì‹ì€ $F_0 = S_0(1 + r_f - d)^T$ ì…ë‹ˆë‹¤"
**ì£¼ì˜**: F0 â†’ F_0 (zero), S0 â†’ S_0 (zero), rf â†’ r_f (letter f)

**ì›ë¬¸**: "If d < rf, futures prices exceed spot prices"
**ì˜¬ë°”ë¥¸ ë³€í™˜**: "$d < r_f$ì¸ ê²½ìš°, ì„ ë¬¼ ê°€ê²©ì´ í˜„ë¬¼ ê°€ê²©ì„ ì´ˆê³¼í•©ë‹ˆë‹¤"

**ì›ë¬¸**: "Calculate: x = (a+b)/c"
**ì˜¬ë°”ë¥¸ ë³€í™˜**: "ê³„ì‚°: $x = \\frac{a+b}{c}$"

**ì›ë¬¸**: "E = mc^2"
**ì˜¬ë°”ë¥¸ ë³€í™˜**: "$E = mc^2$"

**ì›ë¬¸**: "$1,894 - $1,891 = $3.00"
**ì˜¬ë°”ë¥¸ ë³€í™˜**: "$1,894 - $1,891 = $3.00$"
**ì£¼ì˜**: ì‰¼í‘œë¥¼ {,}ë¡œ ë³€í™˜í•˜ì§€ ë§ ê²ƒ!

**ì ˆëŒ€ ê¸ˆì§€**:
- âŒ ë°±í‹± ì‚¬ìš©: \`F0 = S0(1 + rf - d)^T\`
- âŒ ì¼ë°˜ í…ìŠ¤íŠ¸: "F0 = S0(1 + rf - d)^T"
- âŒ ì½”ë“œ ë¸”ë¡: \`\`\`F0 = S0(1 + rf - d)^T\`\`\`
- âŒ HTML: <code>F0 = S0(1 + rf - d)^T</code>

**ë°˜ë“œì‹œ ì‚¬ìš©**:
- âœ… ì¸ë¼ì¸: $F_0 = S_0(1 + r_f - d)^T$
- âœ… ë¸”ë¡: $$F_0 = S_0(1 + r_f - d)^T$$

**í‘œ ë³€í™˜ ê·œì¹™ (ë§¤ìš° ë§¤ìš° ì¤‘ìš” - ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ!)**:

ì›ë¬¸ì— í‘œ í˜•íƒœì˜ ë°ì´í„°ê°€ ìˆìœ¼ë©´ **ë¬´ì¡°ê±´** ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ë³€í™˜:

**ì˜¬ë°”ë¥¸ ì˜ˆì‹œ**:

| Support | Frequency | Proportion |
| :--- | :--- | :--- |
| ì°¨ì€ìš° | 27 | 0.54 |
| ë°•ë³´ê²€ | 23 | 0.46 |

**ë˜ ë‹¤ë¥¸ ì˜ˆì‹œ**:

| í•­ëª© | ê°’ | ë¹„ê³  |
| :--- | :--- | :--- |
| í‰ê·  | 85.3 | ìƒìœ„ 10% |
| í‘œì¤€í¸ì°¨ | 12.7 | - |

**í•„ìˆ˜ ê·œì¹™ (ì ˆëŒ€ ì§€ì¼œì•¼ í•¨!)**:
1. í‘œ ìœ„ì•„ë˜ë¡œ ë°˜ë“œì‹œ **ë¹ˆ ì¤„ 2ê°œ** (ì¤„ë°”ê¿ˆ 2ë²ˆ)
2. **ì²« ë²ˆì§¸ í–‰**: í—¤ë” (| í—¤ë”1 | í—¤ë”2 |)
3. **ë‘ ë²ˆì§¸ í–‰**: ì •ë ¬ êµ¬ë¶„ì„  (| :--- | :--- |) - **ë°˜ë“œì‹œ ì½œë¡ (:) í¬í•¨í•˜ì—¬ ì •ë ¬ ëª…ì‹œ**
4. **ì„¸ ë²ˆì§¸ í–‰ë¶€í„°**: ë°ì´í„° í–‰
5. ê° ì…€ì€ **ë°˜ë“œì‹œ** | ê¸°í˜¸ë¡œ êµ¬ë¶„
6. ëª¨ë“  í–‰ì˜ ì—´ ê°œìˆ˜ **ë™ì¼**í•˜ê²Œ ìœ ì§€
7. **ì ˆëŒ€ ê¸ˆì§€**: í‘œë¥¼ "í•­ëª©1: ê°’1, í•­ëª©2: ê°’2" ê°™ì€ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
8. **ì ˆëŒ€ ê¸ˆì§€**: í‘œë¥¼ ë¦¬ìŠ¤íŠ¸ë‚˜ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

**ì ˆëŒ€ ê¸ˆì§€**:
- âŒ ì›ë¬¸ì— ì—†ëŠ” ì„¤ëª… ì¶”ê°€
- âŒ "ê°œë…:", "ì •ì˜:" ê°™ì€ í…œí”Œë¦¿
- âŒ ì™¸ë¶€ ì§€ì‹ìœ¼ë¡œ ë³´ì¶©
- âŒ í‘œë¥¼ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
- âŒ ìˆ˜ì‹ì„ ë°±í‹±(\`)ì´ë‚˜ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì‘ì„±
- âŒ <mark> íƒœê·¸ë¥¼ ë‹«ì§€ ì•Šê³  </mark>ë§Œ ì‘ì„±
- âŒ <mark>ì™€ ** ë³¼ë“œë¥¼ í•¨ê»˜ ì‚¬ìš© (ì˜ˆ: <mark>**í…ìŠ¤íŠ¸**</mark>) - ì¤‘ìš” í‘œì‹œëŠ” <mark>ë§Œ ì‚¬ìš©í•  ê²ƒ

**ì¶œë ¥**: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ (## ì œëª© í¬í•¨, ì›ë¬¸ ìˆœì„œ ìœ ì§€)`;

        // GPT-5.1: Convert to A mode
        const conversionResponse = await openai.chat.completions.create({
          model: ADVANCED_MODEL,
          messages: [
            {
              role: "system",
              content: "ë‹¹ì‹ ì€ ì •í™•í•œ ë³€í™˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê·œì¹™ì„ ì •í™•íˆ ë”°ë¦…ë‹ˆë‹¤. íŠ¹íˆ:\n1. ëª¨ë“  ìˆ˜ì‹/ê³µì‹ì€ 100% LaTeX í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ($...$, $$...$$)\n2. í‘œëŠ” ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n3. ë°±í‹±(`)ì´ë‚˜ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ìˆ˜ì‹ì„ ì ˆëŒ€ ì‘ì„±í•˜ì§€ ì•ŠìŒ"
            },
            {
              role: "user",
              content: conversionPrompt
            }
          ],
          temperature: 1.0,
          reasoning_effort: "low",
        });

        let converted = conversionResponse.choices[0].message.content!.trim();
        console.log(`[GPT-5.1] Chunk ${i + 1} converted: ${converted.length} chars`);

        // ì˜ëª»ëœ LaTeX ì‰¼í‘œ í‘œê¸°ë²• ìˆ˜ì • (1{,}894 â†’ 1,894)
        converted = converted.replace(/(\d)\{,\}(\d)/g, '$1,$2');

        // ìˆ˜ì‹ ê²€ì¦ ë‹¨ê³„: ë°±í‹±ì´ë‚˜ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ëœ ìˆ˜ì‹ ê°ì§€ ë° ìˆ˜ì •
        const hasInvalidMath = /`[^`]*(?:[=+\-*/^_]|\\frac|\\sum|\\int)[^`]*`/.test(converted) ||
                              /(?:^|[^$])[A-Z]_?\d?\s*=\s*[^$\n]{3,}(?:[+\-*/^]|\\)/.test(converted) ||
                              /_[!"#$%&'()]/.test(converted) || // ì˜ëª»ëœ ì•„ë˜ì²¨ì ê°ì§€ (ì˜ˆ: F_!, S_!, r_"#)
                              /\$[^$]*_[!"#$%&'()][^$]*\$/.test(converted); // LaTeX ë‚´ë¶€ì˜ ì˜ëª»ëœ ì•„ë˜ì²¨ì

        if (hasInvalidMath) {
          console.log(`[MATH-FIX] Chunk ${i + 1} has invalid math formatting, fixing...`);

          const mathFixPrompt = `ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ìˆ˜ì‹ì´ ì˜ëª» í‘œí˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  ìˆ˜ì‹ì„ LaTeX í˜•ì‹ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.

**ì›ë³¸ í…ìŠ¤íŠ¸**:
${converted}

**ìˆ˜ì • ê·œì¹™**:
1. ë°±í‹±(\`)ìœ¼ë¡œ ê°ì‹¼ ìˆ˜ì‹ â†’ LaTeX ($...$)ë¡œ ë³€í™˜
2. ì¼ë°˜ í…ìŠ¤íŠ¸ ìˆ˜ì‹ â†’ LaTeXë¡œ ë³€í™˜
3. **ì˜ëª»ëœ ì•„ë˜ì²¨ì ìˆ˜ì • (ë§¤ìš° ì¤‘ìš”!)**:
   - F_! â†’ F_0 (ìˆ«ì 0)
   - S_! â†’ S_0 (ìˆ«ì 0)
   - r_"# â†’ r_f (ë¬¸ì f)
   - d < r_"# â†’ $d < r_f$
4. ì˜ˆì‹œ:
   - \`F0 = S0(1 + rf - d)^T\` â†’ $F_0 = S_0(1 + r_f - d)^T$
   - $F_! = S_!(1 + r_"#)$ â†’ $F_0 = S_0(1 + r_f)$
   - E = mc^2 â†’ $E = mc^2$
5. ë‚˜ë¨¸ì§€ ë‚´ìš©ì€ ê·¸ëŒ€ë¡œ ìœ ì§€

**ì¶œë ¥**: ìˆ˜ì‹ë§Œ ìˆ˜ì •ëœ ì „ì²´ í…ìŠ¤íŠ¸`;

          const mathFixResponse = await openai.chat.completions.create({
            model: MINI_MODEL,
            messages: [{
              role: "system",
              content: "ë‹¹ì‹ ì€ ìˆ˜ì‹ í‘œê¸°ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ëª¨ë“  ìˆ˜í•™ í‘œí˜„ì„ LaTeX í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
            }, {
              role: "user",
              content: mathFixPrompt
            }],
            temperature: 1.0,
          });

          converted = mathFixResponse.choices[0].message.content!.trim();
          console.log(`[MATH-FIX] Chunk ${i + 1} math fixed`);
        }

        // Now enhance with Mini model
        const enhancementPrompt = `ë‹¹ì‹ ì€ ì‹œí—˜ ì¤€ë¹„ë¥¼ ë•ëŠ” í•™ìŠµ ì½”ì¹˜ì…ë‹ˆë‹¤.

ì•„ë˜ í•™ìŠµ ìë£Œë¥¼ ë¶„ì„í•˜ê³ , ê° ## ì œëª© ì„¹ì…˜ ë§ˆì§€ë§‰ì— **"### ğŸ’¡ í•œê±¸ìŒ ë”!"** ì„¹ì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”.

**ì¤‘ìš” ê·œì¹™**:
1. ì›ë³¸ ë‚´ìš©ì€ ì ˆëŒ€ ìˆ˜ì •í•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ìœ ì§€
2. ê° ## ì œëª© ì„¹ì…˜ ëì—ë§Œ "### ğŸ’¡ í•œê±¸ìŒ ë”!" ì¶”ê°€
3. "í•œê±¸ìŒ ë”!" ë‚´ìš©: ì‹œí—˜ íŒ, ì‹¤ì „ ì‘ìš©, ì•”ê¸° ìš”ë ¹, ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜ ë“±
4. 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±
5. **ë°˜ë“œì‹œ ì›ë³¸ ìë£Œì—ì„œ ìœ ë˜í•œ êµ¬ì²´ì ì¸ íŒë§Œ ì‘ì„±**

**ì ˆëŒ€ ê¸ˆì§€**:
- âŒ "ê°•ì˜ ë…¸íŠ¸ì™€ ì—°ë™í•´ í™•ì¸í•˜ì„¸ìš”" ê°™ì€ ì¼ë°˜ì ì¸ ì¡°ì–¸
- âŒ "êµì¬ì™€ ëŒ€ì¡°í•©ë‹ˆë‹¤" ê°™ì€ í•™ìŠµ ë°©ë²•ë¡ 
- âŒ "ì˜¤íƒ€ë‚˜ ë¶€ì¡±í•œ ë¶€ë¶„ì´ ìˆìœ¼ë©´" ê°™ì€ ë©”íƒ€ ì¡°ì–¸
- âŒ ì›ë³¸ ìë£Œì™€ ë¬´ê´€í•œ ì¼ë°˜ë¡ 
- âŒ ë¹ˆ ê³µê°„ì„ ì±„ìš°ê¸° ìœ„í•œ ì˜ë¯¸ì—†ëŠ” í…ìŠ¤íŠ¸

**í•™ìŠµ ìë£Œ**:
${converted}`;

        const miniResponse = await openai.chat.completions.create({
          model: MINI_MODEL,
          messages: [{ role: "user", content: enhancementPrompt }],
          temperature: 1.0,
        });

        let enhanced = miniResponse.choices[0].message.content!.trim();

        // ë°±í‹± ì½”ë“œ ë¸”ë¡ ì œê±°
        enhanced = enhanced
          .replace(/^```[a-z]*\n/gm, '')
          .replace(/\n```$/gm, '')
          .trim();

        console.log(`[Mini] Chunk ${i + 1} enhanced: ${enhanced.length} chars`);

        return { index: i, content: enhanced };
      });

      // Wait for all conversions AND enhancements to complete
      const convertedChunks = await Promise.all(conversionPromises);

      // Sort by index and join
      convertedChunks.sort((a, b) => a.index - b.index);
      let fullContent = convertedChunks.map(c => c.content).join("\n\n");

      console.log(`[PARALLEL] All chunks converted and enhanced: ${fullContent.length} total chars`);

      // Stream all content at once after parallel completion
      for (let i = 0; i < convertedChunks.length; i++) {
        await sendEvent("content", {
          content: convertedChunks[i].content,
          isComplete: false,
          progress: ((i + 1) / convertedChunks.length) * 100
        });
      }

      // Note: "í•œê±¸ìŒ ë”!" ì„¹ì…˜ì€ ê° ì²­í¬ ë³€í™˜ ì‹œ ì´ë¯¸ ë³‘ë ¬ë¡œ ì¶”ê°€ë¨

      // ====================
      // STEP 5: Update DB with final content
      // ====================
      const totalDuration = Date.now() - startTime;
      const estimatedInputTokens = chunks.reduce((sum, c) => sum + Math.ceil(c.text.length / 4), 0);
      const estimatedOutputTokens = Math.ceil(fullContent.length / 4);
      const estimatedCost =
        (estimatedInputTokens / 1_000_000) * 2.50 +
        (estimatedOutputTokens / 1_000_000) * 10.00;

      const metricsSummary = {
        totalDuration,
        approach: "smart-hybrid",
        apiCalls: chunks.length,
        chunkCount: chunks.length,
        outputLength: fullContent.length,
        estimatedInputTokens,
        estimatedOutputTokens,
        estimatedCost,
        pdfPages: pageCount,
        extractedTextLength: extractedText.length,
      };

      await supabase
        .from("materials")
        .update({
          analysis: {
            content: fullContent,
            metrics: metricsSummary,
            status: 'completed',
            generatedAt: new Date().toISOString(),
          }
        })
        .eq("id", insertedMaterial.id);

      console.log(`[METRICS] Complete: ${(totalDuration / 1000).toFixed(2)}s, ~$${estimatedCost.toFixed(4)}`);

      // ====================
      // STEP 6: Generate Core Concepts (Background)
      // ====================
      console.log('[CONCEPTS] Starting background generation...');

      
      // Generate concepts in background without blocking response
      (async () => {
        try {
          const conceptsPrompt = type === "exam"
            ? `ë‹¤ìŒì€ ì‹œí—˜ ëŒ€ë¹„ í•™ìŠµ ìë£Œì…ë‹ˆë‹¤. í•µì‹¬ ê°œë…ì„ ì¶”ì¶œí•˜ì—¬ ì •ë¦¬í•´ì£¼ì„¸ìš”.

**í•™ìŠµ ìë£Œ ë‚´ìš©**:
${fullContent.substring(0, 30000)}

**ëª©í‘œ**: ì‹œí—˜ì— ê¼­ ë‚˜ì˜¬ í•µì‹¬ ê°œë…ë§Œ ì—„ì„ í•˜ì—¬ ì •ë¦¬

**ì¶”ì¶œ ê·œì¹™**:
1. **ê°œë… ê°œìˆ˜**: 8-12ê°œ (ë§ì§€ ì•Šê²Œ)
2. **ê° ê°œë… êµ¬ì¡°**:
   - **ì œëª©**: í•µì‹¬ í‚¤ì›Œë“œ (3-5ë‹¨ì–´)
   - **ì •ì˜**: 1-2ë¬¸ì¥ìœ¼ë¡œ ëª…í™•í•˜ê²Œ
   - **í•µì‹¬ í¬ì¸íŠ¸**: ì‹œí—˜ì— ë‚˜ì˜¬ ì¤‘ìš” ì‚¬í•­ 2-3ê°œ
   - **ì˜ˆì‹œ/ê³µì‹**: ìˆë‹¤ë©´ LaTeX í˜•ì‹ìœ¼ë¡œ
3. **ì ˆëŒ€ ê¸ˆì§€**: ì›ë¬¸ì— ì—†ëŠ” ë‚´ìš© ì¶”ê°€, ì¼ë°˜ì ì¸ ì¡°ì–¸

**ì¶œë ¥ í˜•ì‹** (ë§ˆí¬ë‹¤ìš´):

## ê°œë… 1: ì œëª©

**ì •ì˜**: ê°„ë‹¨ëª…ë£Œí•œ ì •ì˜

**í•µì‹¬ í¬ì¸íŠ¸**:
- í¬ì¸íŠ¸ 1
- í¬ì¸íŠ¸ 2
- í¬ì¸íŠ¸ 3

**ê³µì‹/ì˜ˆì‹œ**: (ìˆë‹¤ë©´)

---

(ë‹¤ìŒ ê°œë… ë°˜ë³µ)`
            : `ë‹¤ìŒì€ ì—…ë¬´ ìë£Œì…ë‹ˆë‹¤. í•µì‹¬ ê°œë…ì„ ì¶”ì¶œí•˜ì—¬ ì •ë¦¬í•´ì£¼ì„¸ìš”.

**ì—…ë¬´ ìë£Œ ë‚´ìš©**:
${fullContent.substring(0, 30000)}

**ëª©í‘œ**: ì—…ë¬´ì— ê¼­ í•„ìš”í•œ í•µì‹¬ ê°œë…ë§Œ ì •ë¦¬

**ì¶”ì¶œ ê·œì¹™**:
1. **ê°œë… ê°œìˆ˜**: 8-12ê°œ
2. **ê° ê°œë… êµ¬ì¡°**:
   - **ì œëª©**: í•µì‹¬ í‚¤ì›Œë“œ
   - **ì„¤ëª…**: ì‹¤ë¬´ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€
   - **ì£¼ìš” ì‚¬í•­**: ì£¼ì˜í•  ì , ì¤‘ìš” í”„ë¡œì„¸ìŠ¤
   - **ì˜ˆì‹œ**: ìˆë‹¤ë©´ êµ¬ì²´ì ìœ¼ë¡œ
3. **ì ˆëŒ€ ê¸ˆì§€**: ì›ë¬¸ì— ì—†ëŠ” ë‚´ìš© ì¶”ê°€

**ì¶œë ¥ í˜•ì‹** (ë§ˆí¬ë‹¤ìš´):

## ê°œë… 1: ì œëª©

**ì„¤ëª…**: ê°„ë‹¨ëª…ë£Œí•œ ì„¤ëª…

**ì£¼ìš” ì‚¬í•­**:
- ì‚¬í•­ 1
- ì‚¬í•­ 2
- ì‚¬í•­ 3

**ì˜ˆì‹œ**: (ìˆë‹¤ë©´)

---

(ë‹¤ìŒ ê°œë… ë°˜ë³µ)`;

          const conceptsResponse = await openai.chat.completions.create({
            model: MINI_MODEL,
            messages: [{ role: "user", content: conceptsPrompt }],
            temperature: 1.0,
          });

          const conceptsContent = conceptsResponse.choices[0].message.content!.trim()
            .replace(/^```[a-z]*\n/gm, '')
            .replace(/\n```$/gm, '')
            .trim();

          // Get current analysis first
          const { data: currentMaterial } = await supabase
            .from("materials")
            .select("analysis")
            .eq("id", insertedMaterial.id)
            .single();

          await supabase
            .from("materials")
            .update({
              analysis: {
                ...(currentMaterial?.analysis || {}),
                concepts_content: conceptsContent,
              }
            })
            .eq("id", insertedMaterial.id);

          console.log('[CONCEPTS] Background generation completed');
        } catch (error) {
          console.error('[CONCEPTS] Background generation failed:', error);
          // Don't block main flow if concepts generation fails
        }
      })();

      await sendEvent("complete", {
        materialId: insertedMaterial.id,
        content: fullContent,
        metrics: metricsSummary,
      });

      await writer.close();
    } catch (error: any) {
      console.error("[ERROR]", error);
      await sendEvent("error", { error: error.message });
      await writer.close();
    }
  })();

  return new Response(stream.readable, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache",
      "Connection": "keep-alive",
    },
  });
}
