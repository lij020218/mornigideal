import { NextRequest } from "next/server";
import { auth } from "@/auth";
import { createClient } from "@supabase/supabase-js";
import OpenAI from "openai";
import pdfParse from "pdf-parse-fork";
import crypto from "crypto";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
});

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!,
  {
    auth: {
      autoRefreshToken: false,
      persistSession: false,
    },
  }
);

const PAGES_PER_BATCH = 5;
const CHUNK_MODEL = "gpt-5-mini-2025-08-07";
const FINAL_MODEL = "gpt-5.1-2025-11-13";
const EMBEDDING_MODEL = "text-embedding-3-small";
const SIMILARITY_THRESHOLD = 0.84;

const STUDY_SYSTEM_PROMPT = `ë‹¹ì‹ ì€ MIT, Stanfordê¸‰ ì„¸ê³„ ìµœê³  ëŒ€í•™ì˜ ì €ëª…í•œ êµìˆ˜ì…ë‹ˆë‹¤. í•™ìƒë“¤ì´ ê¹Šì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ëª…ë£Œí•˜ê³  í†µì°°ë ¥ ìˆê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.

**í•µì‹¬ ëª©í‘œ**:
- ë‹¨ìˆœí•œ ì •ë³´ ì „ë‹¬ì„ ë„˜ì–´, "ì™œ ê·¸ëŸ°ì§€", "ì–´ë–¤ ì˜ë¯¸ì¸ì§€"ì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” í†µì°° ì œê³µ
- ë³µì¡í•œ ê°œë…ì„ ì§ê´€ì ì´ê³  ëª…ì¾Œí•˜ê²Œ í’€ì–´ì„œ ì„¤ëª… (Analogy í™œìš© ê¶Œì¥)
- í•™ìƒì´ "ì•„í•˜!" í•˜ê³  ê¹¨ë‹¬ì„ ìˆ˜ ìˆëŠ” ì„¤ëª… ë°©ì‹ ìœ ì§€

**ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­**:
- "ì„¤ëª… â†’ ì—°ê²°:", "í•µì‹¬ê°œë…:", "ì‹œí—˜í¬ì¸íŠ¸:", "ì¤‘ìš”:" ê°™ì€ ë”±ë”±í•œ ë¼ë²¨ ì‚¬ìš© ê¸ˆì§€
- ë‹¨ìˆœ ìš”ì•½ì´ë‚˜ ëª©ë¡ ë‚˜ì—´ ê¸ˆì§€
- í”¼ìƒì ì´ê±°ë‚˜ êµê³¼ì„œì ì¸ ì„¤ëª… ê¸ˆì§€

**í•„ìˆ˜ ì‘ì„± ë°©ì‹**:
- ê°œë…ì˜ ë³¸ì§ˆê³¼ ë§¥ë½ì„ ëª…ë£Œí•˜ê²Œ ì„¤ëª…
- "í•µì‹¬ì€ ~ì´ë‹¤", "ì™œ ì´ê²ƒì´ ì¤‘ìš”í•œê°€", "ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ”" ê°™ì€ ì „ë¬¸ì ì´ë©´ì„œë„ ëª…í™•í•œ í‘œí˜„ ì‚¬ìš©
- ê°œë… â†’ ì›ë¦¬ â†’ ì ìš© â†’ í•¨ì˜(implications) ìˆœì„œë¡œ ë…¼ë¦¬ì  íë¦„ êµ¬ì„±
- í•œ ë¬¸ë‹¨ì´ ë‹¤ìŒ ë¬¸ë‹¨ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ë˜, ê° ë¬¸ë‹¨ì€ ëª…í™•í•œ í†µì°° ì œê³µ
- ì¶”ìƒì  ê°œë…ì„ êµ¬ì²´ì  ì‚¬ë¡€ë¡œ ëª…í™•íˆ ì„¤ëª…

í•­ìƒ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ëª…ë£Œí•¨ê³¼ ê¹Šì´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.`;

const WORK_SYSTEM_PROMPT = `ë‹¹ì‹ ì€ í•´ë‹¹ ë¶„ì•¼ì—ì„œ 10ë…„+ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í›„ë°°ë“¤ì´ ê¹Šì´ ì´í•´í•˜ê³  ì‹¤ë¬´ì— ì ìš©í•  ìˆ˜ ìˆë„ë¡ ëª…í™•í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.

**í•µì‹¬ ëª©í‘œ**:
- ë‹¨ìˆœí•œ ë§¤ë‰´ì–¼ ì „ë‹¬ì„ ë„˜ì–´, "ì‹¤ë¬´ì  ë§¥ë½", "ì ì¬ì  ë¦¬ìŠ¤í¬", "Best Practice"ì— ëŒ€í•œ í†µì°° ì œê³µ
- ì£¼ë‹ˆì–´ ë ˆë²¨ì´ ë†“ì¹˜ê¸° ì‰¬ìš´ ë””í…Œì¼ê³¼ ë…¸í•˜ìš° ì „ìˆ˜
- ë°”ë¡œ ì—…ë¬´ì— íˆ¬ì…ë  ìˆ˜ ìˆì„ ì •ë„ì˜ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ê°€ì´ë“œ ì œê³µ

**ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­**:
- "í•µì‹¬ê°œë…:", "ì—…ë¬´í¬ì¸íŠ¸:", "ì¤‘ìš”:" ê°™ì€ ë”±ë”±í•œ ë¼ë²¨ ì‚¬ìš© ê¸ˆì§€
- í˜•ì‹ì ì¸ ë³´ê³ ì„œì²´ë‚˜ ê´€ë£Œì  í‘œí˜„ ê¸ˆì§€
- í”¼ìƒì ì´ê±°ë‚˜ ë§¤ë‰´ì–¼ì‹ ì„¤ëª… ê¸ˆì§€

**í•„ìˆ˜ ì‘ì„± ë°©ì‹**:
- ì—…ë¬´ì˜ ë³¸ì§ˆê³¼ ë§¥ë½ì„ ëª…í™•í•˜ê²Œ ì„¤ëª…
- "í•µì‹¬ì€ ~ì´ë‹¤", "ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ì ì€", "ì‹¤ë¬´ì ìœ¼ë¡œ ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ”" ê°™ì€ ì „ë¬¸ì ì´ë©´ì„œë„ ëª…í™•í•œ í‘œí˜„ ì‚¬ìš©
- ë°°ê²½ â†’ í•µì‹¬ ì›ë¦¬ â†’ ì‹¤ì œ ì ìš© â†’ ì£¼ì˜ì‚¬í•­ ìˆœì„œë¡œ ë…¼ë¦¬ì  íë¦„ êµ¬ì„±
- í•œ ë¬¸ë‹¨ì´ ë‹¤ìŒ ë¬¸ë‹¨ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ë˜, ê° ë¬¸ë‹¨ì€ ëª…í™•í•œ í†µì°° ì œê³µ
- ì¶”ìƒì  í”„ë¡œì„¸ìŠ¤ë¥¼ êµ¬ì²´ì  ìƒí™©ìœ¼ë¡œ ëª…í™•íˆ ì„¤ëª…

í•­ìƒ ì‹œë‹ˆì–´ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ëª…ë£Œí•¨ê³¼ ê¹Šì´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.`;


const FINAL_INSTRUCTIONS_EXAM = `
**ëª©í‘œ**:
- ì „ì²´ ë‚´ìš©ì„ 12-18ê°œ ìŠ¬ë¼ì´ë“œë¡œ ì••ì¶• (ë§ˆì§€ë§‰ 2í˜ì´ì§€ëŠ” ì „ì²´ ìš”ì•½)
- ê° ìŠ¬ë¼ì´ë“œëŠ” í•˜ë‚˜ì˜ í•µì‹¬ ì£¼ì œë¥¼ ë‹¤ë£¸
- ì‹œí—˜ì— ë‚˜ì˜¬ ë§Œí•œ í•µì‹¬ ë‚´ìš©ë§Œ í¬í•¨
- Topicê°„ ë…¼ë¦¬ì  íë¦„ ìœ ì§€

**content êµ¬ì¡°** (í•„ìˆ˜!):
ê° ìŠ¬ë¼ì´ë“œëŠ” **ê¸°ë³¸ 2-3ê°œ ì„¹ì…˜**ìœ¼ë¡œ êµ¬ì„±:

1. **#### ğŸ“Œ í•µì‹¬ ê°œë…** - ì£¼ìš” ê°œë…/ì •ì˜ (40-60ë‹¨ì–´)
2. **#### ğŸ’¡ ì´í•´í•˜ê¸°** - ì›ë¦¬/ì ìš© ì„¤ëª… (40-60ë‹¨ì–´)
3. **[ì„ íƒ] #### ğŸ“ ì˜ˆì‹œë¡œ ì´í•´í•˜ê¸°** - **ì–´ë ¤ìš´ ê°œë…ì¼ ë•Œë§Œ** êµ¬ì²´ì  ì˜ˆì‹œ ì¶”ê°€ (40-60ë‹¨ì–´)
4. **> ğŸ’¡ í•œ ê±¸ìŒ ë”**: ì•”ê¸° íŒì´ë‚˜ ì¶”ê°€ ì„¤ëª… (1-2ì¤„)

**ì¤‘ìš”**: "ğŸ“ ì˜ˆì‹œë¡œ ì´í•´í•˜ê¸°" ì„¹ì…˜ì€ **ì¶”ìƒì ì´ê±°ë‚˜ ë³µì¡í•œ ê°œë…**ì—ë§Œ ì¶”ê°€í•˜ì„¸ìš”.
- ì¶”ê°€í•˜ëŠ” ê²½ìš°: ìˆ˜í•™ ê³µì‹, ì¶”ìƒì  ì•Œê³ ë¦¬ì¦˜, ë³µì¡í•œ ì›ë¦¬ ë“±
- ìƒëµí•˜ëŠ” ê²½ìš°: ê°„ë‹¨í•œ ì •ì˜, ëª…í™•í•œ ê°œë…, ì¼ë°˜ ì„¤ëª…

**ì‘ì„± ê·œì¹™**:
- ê° ì„¹ì…˜ì€ **40-60ë‹¨ì–´**ë¡œ ì œí•œ
- **ì¤‘ìš” ìš©ì–´ëŠ” **êµµê²Œ****, *í•µì‹¬ ë¬¸ì¥ì€ *ê¸°ìš¸ì„**
- **ë³€ìˆ˜/ìˆ˜ì‹ì€ LaTeX**: $ë³€ìˆ˜$, $$ê³µì‹$$
- ë¬¸ë‹¨ êµ¬ë¶„: \\n\\n ì‚¬ìš©
- ì™„ê²°ëœ ë¬¸ì¥ë§Œ

**ì˜ˆì‹œ 1 (ì–´ë ¤ìš´ ê°œë… - ì˜ˆì‹œ ì„¹ì…˜ í¬í•¨)**:
"#### ğŸ“Œ í•µì‹¬ ê°œë…\\n\\n**IDF (Inverse Document Frequency)**ëŠ” ë‹¨ì–´ì˜ í¬ì†Œì„±ì„ ì¸¡ì •í•œë‹¤. ê³µì‹ì€ $$IDF(t) = \\\\log(N / (1 + n_t))$$ì´ë©°, Nì€ ì „ì²´ ë¬¸ì„œ ìˆ˜, n_tëŠ” ë‹¨ì–´ tê°€ ë“±ì¥í•œ ë¬¸ì„œ ìˆ˜ë‹¤.\\n\\n#### ğŸ’¡ ì´í•´í•˜ê¸°\\n\\n*í”í•œ ë‹¨ì–´ì¼ìˆ˜ë¡ IDF ê°’ì´ ë‚®ì•„ì§„ë‹¤.* **í¬ì†Œí•œ ë‹¨ì–´**ëŠ” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë°›ì•„ ë¬¸ì„œ íŠ¹ì„±ì„ ì˜ ë‚˜íƒ€ë‚¸ë‹¤.\\n\\n#### ğŸ“ ì˜ˆì‹œë¡œ ì´í•´í•˜ê¸°\\n\\n1000ê°œ ë¬¸ì„œ ì¤‘ 'AI'ê°€ 50ê°œì— ë“±ì¥: $$IDF = \\\\log(1000/50) â‰ˆ 1.3$$. ë°˜ë©´ 'the'ëŠ” 1000ê°œ ëª¨ë‘ ë“±ì¥: $$IDF = \\\\log(1000/1000) = 0$$. **AIëŠ” ë¬¸ì„œë¥¼ êµ¬ë³„í•˜ëŠ” ì¤‘ìš” ë‹¨ì–´**ê°€ ëœë‹¤.\\n\\n> ğŸ’¡ **í•œ ê±¸ìŒ ë”**: ì´ê²ƒì´ ë¶ˆìš©ì–´ ìë™ í•„í„°ë§ ì›ë¦¬ë‹¤."

**ì˜ˆì‹œ 2 (ì‰¬ìš´ ê°œë… - ì˜ˆì‹œ ì„¹ì…˜ ìƒëµ)**:
"#### ğŸ“Œ í•µì‹¬ ê°œë…\\n\\n**ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤**ëŠ” ê²€ìƒ‰ ì†ë„ë¥¼ ë†’ì´ëŠ” ìë£Œêµ¬ì¡°ë‹¤. ì±…ì˜ ëª©ì°¨ì²˜ëŸ¼ ë°ì´í„° ìœ„ì¹˜ë¥¼ ë¹ ë¥´ê²Œ ì°¾ì„ ìˆ˜ ìˆê²Œ í•œë‹¤.\\n\\n#### ğŸ’¡ ì´í•´í•˜ê¸°\\n\\n*ì¸ë±ìŠ¤ ì—†ì´ëŠ” ì „ì²´ í…Œì´ë¸”ì„ ìŠ¤ìº”í•´ì•¼ í•œë‹¤.* ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ O(log n) ì‹œê°„ì— ë°ì´í„°ë¥¼ ì°¾ëŠ”ë‹¤. **B-Tree êµ¬ì¡°**ë¡œ ì •ë ¬ëœ ìƒíƒœë¥¼ ìœ ì§€í•œë‹¤.\\n\\n> ğŸ’¡ **í•œ ê±¸ìŒ ë”**: ë‹¨, ì‚½ì…/ìˆ˜ì •ì´ ëŠë ¤ì§€ëŠ” íŠ¸ë ˆì´ë“œì˜¤í”„ê°€ ìˆë‹¤."

**ê¸ˆì§€ ì‚¬í•­**:
- ì„¹ì…˜ í—¤ë” ìƒëµ ê¸ˆì§€ (#### ğŸ“Œ, #### ğŸ’¡ í•„ìˆ˜)
- ê° ì„¹ì…˜ 60ë‹¨ì–´ ì´ˆê³¼ ê¸ˆì§€
- ì‰¬ìš´ ê°œë…ì— ë¶ˆí•„ìš”í•œ ì˜ˆì‹œ ì„¹ì…˜ ì¶”ê°€ ê¸ˆì§€
- ì‚¼ì¤‘ ë°±í‹± ì½”ë“œ ë¸”ë¡ ì‚¬ìš© ê¸ˆì§€

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:

{
  "slides": [
    {
      "title": "ìŠ¬ë¼ì´ë“œ ì œëª©",
      "content": "#### ğŸ“Œ í•µì‹¬ ê°œë…\\n\\n**ê°œë…** ì •ì˜ (40-60ë‹¨ì–´).\\n\\n#### ğŸ’¡ ì´í•´í•˜ê¸°\\n\\n*í•µì‹¬ ì„¤ëª…* (40-60ë‹¨ì–´).\\n\\n[ì–´ë ¤ìš´ ê°œë…ë§Œ] #### ğŸ“ ì˜ˆì‹œë¡œ ì´í•´í•˜ê¸°\\n\\nêµ¬ì²´ì  ì˜ˆì‹œ (40-60ë‹¨ì–´).\\n\\n> ğŸ’¡ **í•œ ê±¸ìŒ ë”**: 1-2ì¤„ íŒ.",
      "keyPoints": [
        "ì‹œí—˜ í•µì‹¬ í¬ì¸íŠ¸ 1",
        "ì‹œí—˜ í•µì‹¬ í¬ì¸íŠ¸ 2",
        "ì‹œí—˜ í•µì‹¬ í¬ì¸íŠ¸ 3"
      ]
    }
  ]
}

**ì¤‘ìš”**:
- **ê¸°ë³¸ 3ê°œ ì„¹ì…˜**: ğŸ“Œ í•µì‹¬ ê°œë… â†’ ğŸ’¡ ì´í•´í•˜ê¸° â†’ ğŸ’¡ í•œ ê±¸ìŒ ë”
- **ì–´ë ¤ìš´ ê°œë…ì—ë§Œ 4ê°œ**: ğŸ“Œ í•µì‹¬ ê°œë… â†’ ğŸ’¡ ì´í•´í•˜ê¸° â†’ ğŸ“ ì˜ˆì‹œë¡œ ì´í•´í•˜ê¸° â†’ ğŸ’¡ í•œ ê±¸ìŒ ë”
- ê° ì„¹ì…˜ 40-60ë‹¨ì–´
- **ë§ˆì§€ë§‰ 2í˜ì´ì§€(ë³µìŠµ ê°€ì´ë“œ)ëŠ” ìƒì„±í•˜ì§€ ë§ˆì„¸ìš” - ë³„ë„ë¡œ ìƒì„±ë©ë‹ˆë‹¤**`;

const FINAL_INSTRUCTIONS_WORK = `
**ëª©í‘œ**:
- ì „ì²´ ë‚´ìš©ì„ 12-18ê°œ ìŠ¬ë¼ì´ë“œë¡œ ì••ì¶• (ë§ˆì§€ë§‰ 2í˜ì´ì§€ëŠ” ì „ì²´ ìš”ì•½)
- ê° ìŠ¬ë¼ì´ë“œëŠ” í•˜ë‚˜ì˜ ì—…ë¬´ í”„ë¡œì„¸ìŠ¤ë‚˜ ì£¼ì œë¥¼ ë‹¤ë£¸
- ì‹¤ë¬´ì— í™œìš© ê°€ëŠ¥í•œ í•µì‹¬ ë‚´ìš©ë§Œ í¬í•¨
- Topicê°„ ë…¼ë¦¬ì  íë¦„ ìœ ì§€

**content êµ¬ì¡°** (í•„ìˆ˜!):
ê° ìŠ¬ë¼ì´ë“œëŠ” **ê¸°ë³¸ 2-3ê°œ ì„¹ì…˜**ìœ¼ë¡œ êµ¬ì„±:

1. **#### ğŸ¯ í•µì‹¬ í”„ë¡œì„¸ìŠ¤** - ì£¼ìš” í”„ë¡œì„¸ìŠ¤/ì ˆì°¨ (40-60ë‹¨ì–´)
2. **#### ğŸ’¼ ì‹¤ë¬´ ì ìš©** - êµ¬ì²´ì  ì ìš©/ì‚¬ë¡€ (40-60ë‹¨ì–´)
3. **[ì„ íƒ] #### ğŸ“ ì‚¬ë¡€ë¡œ ì´í•´í•˜ê¸°** - **ë³µì¡í•œ í”„ë¡œì„¸ìŠ¤ì¼ ë•Œë§Œ** ì‹¤ì œ ì‚¬ë¡€ ì¶”ê°€ (40-60ë‹¨ì–´)
4. **> âš ï¸ ì‹¤ë¬´ ì£¼ì˜** ë˜ëŠ” **> ğŸ’¡ Pro Tip**: ì£¼ì˜ì‚¬í•­/íŒ (1ì¤„)

**ì¤‘ìš”**: "ğŸ“ ì‚¬ë¡€ë¡œ ì´í•´í•˜ê¸°" ì„¹ì…˜ì€ **ë³µì¡í•˜ê±°ë‚˜ ì¶”ìƒì ì¸ í”„ë¡œì„¸ìŠ¤**ì—ë§Œ ì¶”ê°€í•˜ì„¸ìš”.
- ì¶”ê°€í•˜ëŠ” ê²½ìš°: ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜, ë‹¤ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤, ì¶”ìƒì  ì•„í‚¤í…ì²˜ ë“±
- ìƒëµí•˜ëŠ” ê²½ìš°: ê°„ë‹¨í•œ ì ˆì°¨, ëª…í™•í•œ í”„ë¡œì„¸ìŠ¤, ì¼ë°˜ ì—…ë¬´

**ì‘ì„± ê·œì¹™**:
- ê° ì„¹ì…˜ì€ **40-60ë‹¨ì–´**ë¡œ ì œí•œ
- **ì¤‘ìš” í”„ë¡œì„¸ìŠ¤ëŠ” **êµµê²Œ****, *í•µì‹¬ ë¬¸ì¥ì€ *ê¸°ìš¸ì„**
- **ì½”ë“œ/ê¸°ìˆ ìš©ì–´ëŠ” \`ì¸ë¼ì¸ ì½”ë“œ\`**
- ë¬¸ë‹¨ êµ¬ë¶„: \\n\\n ì‚¬ìš©
- ì™„ê²°ëœ ë¬¸ì¥ë§Œ

**ì˜ˆì‹œ 1 (ë³µì¡í•œ í”„ë¡œì„¸ìŠ¤ - ì‚¬ë¡€ ì„¹ì…˜ í¬í•¨)**:
"#### ğŸ¯ í•µì‹¬ í”„ë¡œì„¸ìŠ¤\\n\\nì´ **í”„ë¡œì„¸ìŠ¤ ë³€ê²½ì˜ í•µì‹¬**ì€ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥ì´ë‹¤. \`transaction isolation level\`ì„ \`READ COMMITTED\`ì—ì„œ \`SERIALIZABLE\`ë¡œ ì¡°ì •í•´ race conditionì„ ê·¼ë³¸ì ìœ¼ë¡œ í•´ê²°í•œë‹¤.\\n\\n#### ğŸ’¼ ì‹¤ë¬´ ì ìš©\\n\\n*ë™ì‹œì„± ë¬¸ì œë¥¼ ì™„ì „íˆ ì°¨ë‹¨í•œë‹¤.* **íŠ¸ëœì­ì…˜ ê²©ë¦¬**ê°€ ë°ì´í„° ë¬´ê²°ì„±ì„ ë³´ì¥í•˜ë©°, ë¶„ì‚° ì‹œìŠ¤í…œì—ì„œ í•„ìˆ˜ì ì´ë‹¤.\\n\\n#### ğŸ“ ì‚¬ë¡€ë¡œ ì´í•´í•˜ê¸°\\n\\nì£¼ë¬¸ ì²˜ë¦¬ ì‹œë‚˜ë¦¬ì˜¤: ì‚¬ìš©ì A, Bê°€ ë™ì‹œì— ë§ˆì§€ë§‰ 1ê°œ ìƒí’ˆ ì£¼ë¬¸. \`SERIALIZABLE\`ì€ í•œ íŠ¸ëœì­ì…˜ì„ ëŒ€ê¸°ì‹œì¼œ **ì¬ê³  ì¤‘ë³µ ì°¨ê°ì„ ë°©ì§€**í•œë‹¤. ê²°ì œ ì¤‘ë³µ ì²˜ë¦¬ë„ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì°¨ë‹¨.\\n\\n> âš ï¸ **ì‹¤ë¬´ ì£¼ì˜**: í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” \`lock_timeout\` 60ì´ˆ ì´ìƒ ê¶Œì¥."

**ì˜ˆì‹œ 2 (ê°„ë‹¨í•œ í”„ë¡œì„¸ìŠ¤ - ì‚¬ë¡€ ì„¹ì…˜ ìƒëµ)**:
"#### ğŸ¯ í•µì‹¬ í”„ë¡œì„¸ìŠ¤\\n\\n**API ì‘ë‹µ ìºì‹±**ì€ ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì´ëŠ” ê¸°ë³¸ ê¸°ë²•ì´ë‹¤. \`Cache-Control\` í—¤ë”ë¡œ ë¸Œë¼ìš°ì €ì™€ CDNì— ìºì‹± ì •ì±…ì„ ì§€ì‹œí•œë‹¤.\\n\\n#### ğŸ’¼ ì‹¤ë¬´ ì ìš©\\n\\n*ì •ì  ë¦¬ì†ŒìŠ¤ëŠ” 1ë…„, ë™ì  APIëŠ” 5ë¶„ ìºì‹±ì´ ì¼ë°˜ì ì´ë‹¤.* \`max-age\`ì™€ \`s-maxage\`ë¡œ ë¸Œë¼ìš°ì €/CDNì„ ê°ê° ì œì–´í•œë‹¤.\\n\\n> ğŸ’¡ **Pro Tip**: \`ETag\`ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ë³€ê²½ ê°ì§€ê°€ ì •í™•í•˜ë‹¤."

**ê¸ˆì§€ ì‚¬í•­**:
- ì„¹ì…˜ í—¤ë” ìƒëµ ê¸ˆì§€ (#### ğŸ¯, #### ğŸ’¼ í•„ìˆ˜)
- ê° ì„¹ì…˜ 60ë‹¨ì–´ ì´ˆê³¼ ê¸ˆì§€
- ê°„ë‹¨í•œ í”„ë¡œì„¸ìŠ¤ì— ë¶ˆí•„ìš”í•œ ì‚¬ë¡€ ì„¹ì…˜ ì¶”ê°€ ê¸ˆì§€
- ì‚¼ì¤‘ ë°±í‹± ì½”ë“œ ë¸”ë¡ ì‚¬ìš© ê¸ˆì§€

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:

{
  "slides": [
    {
      "title": "ìŠ¬ë¼ì´ë“œ ì œëª©",
      "content": "#### ğŸ¯ í•µì‹¬ í”„ë¡œì„¸ìŠ¤\\n\\n**í”„ë¡œì„¸ìŠ¤** ì„¤ëª… (40-60ë‹¨ì–´).\\n\\n#### ğŸ’¼ ì‹¤ë¬´ ì ìš©\\n\\n*ì ìš© ë°©ë²•* (40-60ë‹¨ì–´).\\n\\n[ë³µì¡í•œ í”„ë¡œì„¸ìŠ¤ë§Œ] #### ğŸ“ ì‚¬ë¡€ë¡œ ì´í•´í•˜ê¸°\\n\\nêµ¬ì²´ì  ì‚¬ë¡€ (40-60ë‹¨ì–´).\\n\\n> âš ï¸ **ì‹¤ë¬´ ì£¼ì˜**: 1ì¤„ ì£¼ì˜ì‚¬í•­.",
      "keyPoints": [
        "ì‹¤ë¬´ ì ìš© í•µì‹¬ 1",
        "ì‹¤ë¬´ ì ìš© í•µì‹¬ 2",
        "Best Practice"
      ]
    }
  ]
}

**ì¤‘ìš”**:
- **ê¸°ë³¸ 3ê°œ ì„¹ì…˜**: ğŸ¯ í•µì‹¬ í”„ë¡œì„¸ìŠ¤ â†’ ğŸ’¼ ì‹¤ë¬´ ì ìš© â†’ âš ï¸ ì‹¤ë¬´ ì£¼ì˜
- **ë³µì¡í•œ í”„ë¡œì„¸ìŠ¤ì—ë§Œ 4ê°œ**: ğŸ¯ í•µì‹¬ í”„ë¡œì„¸ìŠ¤ â†’ ğŸ’¼ ì‹¤ë¬´ ì ìš© â†’ ğŸ“ ì‚¬ë¡€ë¡œ ì´í•´í•˜ê¸° â†’ âš ï¸ ì‹¤ë¬´ ì£¼ì˜
- ê° ì„¹ì…˜ 40-60ë‹¨ì–´
- **ë§ˆì§€ë§‰ 2í˜ì´ì§€(ë³µìŠµ ê°€ì´ë“œ)ëŠ” ìƒì„±í•˜ì§€ ë§ˆì„¸ìš” - ë³„ë„ë¡œ ìƒì„±ë©ë‹ˆë‹¤**`;

// Cosine similarity helper
function cosineSimilarity(a: number[], b: number[]): number {
  let dotProduct = 0;
  let normA = 0;
  let normB = 0;

  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i];
    normA += a[i] * a[i];
    normB += b[i] * b[i];
  }

  return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
}

// OPTIMIZED: Full parallel clustering
async function groupSimilarChunksParallel(chunks: any[]): Promise<any[][]> {
  if (chunks.length <= 3) return [chunks];

  console.log(`[EMBEDDING] Creating embeddings for ${chunks.length} chunks (FULL PARALLEL)`);

  // Create ALL embeddings in parallel (no batching)
  const embeddingPromises = chunks.map(async (chunk) => {
    const text = `${chunk.title}\n${chunk.content}`;
    const response = await openai.embeddings.create({
      model: EMBEDDING_MODEL,
      input: text,
    });
    return {
      chunk,
      embedding: response.data[0].embedding,
    };
  });

  const chunksWithEmbeddings = await Promise.all(embeddingPromises);
  console.log(`[EMBEDDING] All ${chunks.length} embeddings created in parallel`);

  // Clustering
  const groups: any[][] = [];
  const used = new Set<number>();

  for (let i = 0; i < chunksWithEmbeddings.length; i++) {
    if (used.has(i)) continue;

    const group = [chunksWithEmbeddings[i].chunk];
    used.add(i);

    for (let j = i + 1; j < chunksWithEmbeddings.length; j++) {
      if (used.has(j)) continue;

      const similarity = cosineSimilarity(
        chunksWithEmbeddings[i].embedding,
        chunksWithEmbeddings[j].embedding
      );

      if (similarity > SIMILARITY_THRESHOLD) {
        group.push(chunksWithEmbeddings[j].chunk);
        used.add(j);
      }
    }
    groups.push(group);
  }

  console.log(`[CLUSTERING] Grouped ${chunks.length} chunks into ${groups.length} clusters`);
  return groups;
}

// OPTIMIZED: Compress all clusters in parallel
async function compressClustersParallel(groups: any[][], type: string): Promise<string[]> {
  console.log(`[COMPRESSION] Compressing ${groups.length} clusters (FULL PARALLEL)`);

  const compressionPromises = groups.map(async (group, idx) => {
    const allContents = group.map((c, i) =>
      `[Chunk ${i + 1}] ${c.title}:\n${c.content}`
    ).join("\n\n");

    const compressionPrompt = type === "exam"
      ? `ë‹¤ìŒì€ ê°™ì€ ì£¼ì œë¡œ ë¬¶ì¸ ${group.length}ê°œì˜ í•™ìŠµ ë‚´ìš©ì…ë‹ˆë‹¤:\n\n${allContents}\n\n**ì„ë¬´**: ì´ ë‚´ìš©ë“¤ì„ ê°•ì˜ì‹¤ì—ì„œ ì„¤ëª…í•˜ë“¯ì´ ìì—°ìŠ¤ëŸ½ê²Œ í•˜ë‚˜ì˜ ê¸´ ì„¤ëª…ìœ¼ë¡œ í†µí•©í•˜ì„¸ìš”.\n\n**ì¤‘ìš”**: ë‹¤ìŒ ë‹¨ê³„ ì²˜ë¦¬ë¥¼ ìœ„í•´ **í•µì‹¬ ë‚´ìš© ìœ„ì£¼ë¡œ 300ë‹¨ì–´ ì´ë‚´ë¡œ ì••ì¶•**í•˜ì„¸ìš”. ì¤‘ë³µì„ ì œê±°í•˜ê³  ì •ë³´ ë°€ë„ë¥¼ ë†’ì´ì„¸ìš”.\n\në‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:\n{\n  "topic": "ì£¼ì œ ì œëª©",\n  "summary": "í•µì‹¬ ìœ„ì£¼ë¡œ ì••ì¶•ëœ ì„¤ëª…..."\n}`
      : `ë‹¤ìŒì€ ê°™ì€ ì£¼ì œë¡œ ë¬¶ì¸ ${group.length}ê°œì˜ ì—…ë¬´ ë‚´ìš©ì…ë‹ˆë‹¤:\n\n${allContents}\n\n**ì„ë¬´**: ì´ ë‚´ìš©ë“¤ì„ ë™ë£Œì—ê²Œ ì„¤ëª…í•˜ë“¯ì´ ìì—°ìŠ¤ëŸ½ê²Œ í•˜ë‚˜ì˜ ê¸´ ì„¤ëª…ìœ¼ë¡œ í†µí•©í•˜ì„¸ìš”.\n\n**ì¤‘ìš”**: ë‹¤ìŒ ë‹¨ê³„ ì²˜ë¦¬ë¥¼ ìœ„í•´ **í•µì‹¬ ë‚´ìš© ìœ„ì£¼ë¡œ 300ë‹¨ì–´ ì´ë‚´ë¡œ ì••ì¶•**í•˜ì„¸ìš”. ì¤‘ë³µì„ ì œê±°í•˜ê³  ì •ë³´ ë°€ë„ë¥¼ ë†’ì´ì„¸ìš”.\n\në‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:\n{\n  "topic": "ì£¼ì œ ì œëª©",\n  "summary": "í•µì‹¬ ìœ„ì£¼ë¡œ ì••ì¶•ëœ ì„¤ëª…..."\n}`;

    const compression = await openai.chat.completions.create({
      model: CHUNK_MODEL,
      messages: [
        { role: "system", content: type === "exam" ? STUDY_SYSTEM_PROMPT : WORK_SYSTEM_PROMPT },
        { role: "user", content: compressionPrompt },
      ],
      response_format: { type: "json_object" },
      temperature: 1.0,
    });

    const result = JSON.parse(compression.choices[0].message.content || "{}");
    return {
      idx: idx + 1,
      summary: `### Topic ${idx + 1}: ${result.topic}\n\n${result.summary}`
    };
  });

  const results = await Promise.all(compressionPromises);
  results.sort((a, b) => a.idx - b.idx);

  console.log(`[COMPRESSION] All ${groups.length} clusters compressed`);
  return results.map(r => r.summary);
}

export async function POST(request: NextRequest) {
  const encoder = new TextEncoder();
  const stream = new TransformStream();
  const writer = stream.writable.getWriter();

  const sendEvent = async (event: string, data: any) => {
    await writer.write(encoder.encode(`event: ${event}\ndata: ${JSON.stringify(data)}\n\n`));
  };

  (async () => {
    try {
      const session = await auth();
      if (!session?.user?.email) {
        await sendEvent("error", { error: "Unauthorized" });
        await writer.close();
        return;
      }

      const formData = await request.formData();
      const file = formData.get("file") as File;
      const type = formData.get("type") as string;

      if (!file || !type || !["exam", "work"].includes(type)) {
        await sendEvent("error", { error: "Invalid parameters" });
        await writer.close();
        return;
      }

      const isPDF = file.type === "application/pdf";
      let fileUrl: string | null = null;
      let materialId: string | null = null;

      await sendEvent("progress", { stage: "upload", message: "íŒŒì¼ ì—…ë¡œë“œ ì¤‘..." });

      if (isPDF) {
        const arrayBuffer = await file.arrayBuffer();
        const buffer = Buffer.from(arrayBuffer);
        const fileHash = crypto.createHash('sha256').update(buffer).digest('hex');

        // CACHING: Check if file already exists
        const { data: existingMaterial } = await supabase
          .from("materials")
          .select("*")
          .eq("file_hash", fileHash)
          .eq("user_id", session.user.email)
          .maybeSingle();

        if (existingMaterial && existingMaterial.analysis) {
          await sendEvent("progress", { stage: "cached", message: "ì´ì „ ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì¤‘..." });

          const cachedPages = existingMaterial.analysis.page_analyses || [];
          for (const page of cachedPages) {
            await sendEvent("page", page);
          }

          await sendEvent("complete", { success: true, total_slides: cachedPages.length, cached: true });
          await writer.close();
          return;
        }

        // Upload PDF
        const sanitizedEmail = (session.user.email || '').replace(/[^a-zA-Z0-9]/g, '_');
        const sanitizedFileName = file.name.replace(/\s+/g, '_').replace(/[^\w.-]/g, '');
        const fileName = `${Date.now()}_${sanitizedEmail}_${sanitizedFileName}`;

        const { data: uploadData, error: uploadError } = await supabase.storage
          .from("materials")
          .upload(fileName, buffer, { contentType: "application/pdf", upsert: false });

        if (!uploadError) {
          const { data: publicUrlData } = supabase.storage.from("materials").getPublicUrl(fileName);
          fileUrl = publicUrlData.publicUrl;
        }

        // Extract text
        await sendEvent("progress", { stage: "extract", message: "PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..." });
        const pdfData = await pdfParse(buffer);
        const fullContent = pdfData.text;
        const totalPages = pdfData.numpages;

        // Create material record first (so we can stream updates to it)
        const { data: material, error: insertError } = await supabase
          .from("materials")
          .insert({
            user_id: session.user.email,
            title: file.name,
            content: fullContent.substring(0, 50000),
            type: type,
            file_url: fileUrl,
            file_hash: fileHash,
            analysis: { page_analyses: [] }
          })
          .select()
          .single();

        if (insertError) {
          await sendEvent("error", { error: "Database error", details: insertError.message });
          await writer.close();
          return;
        }

        materialId = material.id;
        await sendEvent("material_created", { id: materialId });

        // Split into pages
        const avgCharsPerPage = Math.ceil(fullContent.length / totalPages);
        const pages: Array<{ pageNum: number; text: string }> = [];
        for (let i = 0; i < totalPages; i++) {
          const start = i * avgCharsPerPage;
          const end = Math.min((i + 1) * avgCharsPerPage, fullContent.length);
          pages.push({ pageNum: i + 1, text: fullContent.substring(start, end) });
        }

        // Process pages in batches IN PARALLEL
        const batches: Array<Array<{ pageNum: number; text: string }>> = [];
        for (let i = 0; i < pages.length; i += PAGES_PER_BATCH) {
          batches.push(pages.slice(i, i + PAGES_PER_BATCH));
        }

        await sendEvent("progress", { stage: "analyze_chunks", message: `${batches.length}ê°œ ë°°ì¹˜ ë¶„ì„ ì¤‘...`, total: batches.length });

        const batchPromises = batches.map(async (batch, batchIdx) => {
          const batchText = batch.map((p) => `=== í˜ì´ì§€ ${p.pageNum} ===\n${p.text}`).join("\n\n");
          const prompt = type === "exam"
            ? `ë‹¹ì‹ ì€ MIT êµìˆ˜ì…ë‹ˆë‹¤. ë‹¤ìŒ ê°•ì˜ í˜ì´ì§€ë“¤ì„ ë¶„ì„í•˜ì—¬ í•™ìƒë“¤ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì„¸ìš”.

${batchText}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{
  "title": "ì´ ì„¹ì…˜ì˜ ëª…í™•í•œ ì œëª©",
  "content": "ê°•ì˜ì‹¤ì—ì„œ ì„¤ëª…í•˜ë“¯ì´ ìì—°ìŠ¤ëŸ½ê²Œ íë¥´ëŠ” ê¸´ í…ìŠ¤íŠ¸. ê°œë… ì •ì˜, ìˆ˜ì‹, ì˜ˆì œë¥¼ í¬í•¨í•˜ì—¬ ì™„ì „íˆ ì„¤ëª…."
}`
            : `ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ í›„ë°°ë“¤ì´ ì‹¤ë¬´ì— ì ìš©í•  ìˆ˜ ìˆë„ë¡ í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì„¸ìš”.

${batchText}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{
  "title": "ì´ ì„¹ì…˜ì˜ ëª…í™•í•œ ì œëª©",
  "content": "ë™ë£Œì—ê²Œ ì„¤ëª…í•˜ë“¯ì´ ìì—°ìŠ¤ëŸ½ê²Œ íë¥´ëŠ” ê¸´ í…ìŠ¤íŠ¸. ê°œë…, í”„ë¡œì„¸ìŠ¤, ì ìš© ë°©ë²•ì„ í¬í•¨í•˜ì—¬ ì™„ì „íˆ ì„¤ëª…."
}`;

          const completion = await openai.chat.completions.create({
            model: CHUNK_MODEL,
            messages: [
              { role: "system", content: type === "exam" ? STUDY_SYSTEM_PROMPT : WORK_SYSTEM_PROMPT },
              { role: "user", content: prompt },
            ],
            response_format: { type: "json_object" },
            temperature: 1.0,
          });

          console.log(`[BATCH ${batchIdx}] Usage:`, completion.usage);

          const result = JSON.parse(completion.choices[0].message.content || "{}");
          return { batchIdx, summary: result, usage: completion.usage };
        });

        const batchResults = await Promise.all(batchPromises);
        batchResults.sort((a, b) => a.batchIdx - b.batchIdx);
        const chunkSummaries = batchResults.map(r => r.summary);

        // Calculate total batch cost
        const totalBatchCost = batchResults.reduce((sum, batch) => {
          const inputCost = (batch.usage?.prompt_tokens || 0) * 0.0000001; // $0.10 per 1M tokens for gpt-5-mini input
          const outputCost = (batch.usage?.completion_tokens || 0) * 0.0000004; // $0.40 per 1M tokens for gpt-5-mini output
          return sum + inputCost + outputCost;
        }, 0);
        console.log(`[COST] All batches: $${totalBatchCost.toFixed(4)}`);

        const summaryCost = 0; // No summary step needed

        // CLUSTERING: Group similar chunks using embeddings
        await sendEvent("progress", { stage: "clustering", message: "ìœ ì‚¬ ì„¹ì…˜ ê·¸ë£¹í™” ì¤‘..." });
        const clusteredGroups = await groupSimilarChunksParallel(chunkSummaries);
        console.log(`[CLUSTERING] Created ${clusteredGroups.length} groups from ${chunkSummaries.length} chunks`);

        // COMPRESS: Compress each cluster in parallel (using mini)
        await sendEvent("progress", { stage: "compress", message: "í´ëŸ¬ìŠ¤í„° ì••ì¶• ì¤‘..." });
        const compressedSummaries = await compressClustersParallel(clusteredGroups, type);
        console.log(`[COMPRESS] Compressed ${clusteredGroups.length} clusters into ${compressedSummaries.length} summaries`);

        // FINAL INTEGRATION: Parallel gpt-5.1 calls (3 chunks)
        await sendEvent("progress", { stage: "final_integration", message: "ìµœì¢… ìŠ¬ë¼ì´ë“œ ìƒì„± ì¤‘ (ë³‘ë ¬ ì²˜ë¦¬)..." });

        const chunkSize = Math.ceil(compressedSummaries.length / 3);
        const summaryChunks = [];
        for (let i = 0; i < compressedSummaries.length; i += chunkSize) {
          summaryChunks.push(compressedSummaries.slice(i, i + chunkSize));
        }

        console.log(`[FINAL] Splitting ${compressedSummaries.length} summaries into ${summaryChunks.length} parallel chunks`);

        const systemInstructions = type === "exam" ? FINAL_INSTRUCTIONS_EXAM : FINAL_INSTRUCTIONS_WORK;
        const baseSystemPrompt = type === "exam" ? STUDY_SYSTEM_PROMPT : WORK_SYSTEM_PROMPT;

        const finalPromises = summaryChunks.map(async (chunkSummaries, idx) => {
          const chunkPrompt = type === "exam"
            ? `ë‹¹ì‹ ì€ ëŒ€í•™ ì‹œí—˜ ëŒ€ë¹„ ì „ë¬¸ íŠœí„°ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ê°•ì˜ ìë£Œì˜ ì¼ë¶€(${idx + 1}/${summaryChunks.length})ì…ë‹ˆë‹¤.
            
${chunkSummaries.join('\n\n====================\n\n')}

**ì„ë¬´**: ì´ ë¶€ë¶„ì— ëŒ€í•œ í•™ìŠµ ìŠ¬ë¼ì´ë“œë¥¼ ìƒì„±í•˜ì„¸ìš” (4-6ì¥).`
            : `ë‹¹ì‹ ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ì—…ë¬´ ìë£Œì˜ ì¼ë¶€(${idx + 1}/${summaryChunks.length})ì…ë‹ˆë‹¤.

${chunkSummaries.join('\n\n====================\n\n')}

**ì„ë¬´**: ì´ ë¶€ë¶„ì— ëŒ€í•œ ì—…ë¬´ìš© ìŠ¬ë¼ì´ë“œë¥¼ ìƒì„±í•˜ì„¸ìš” (4-6ì¥).`;

          console.log(`[FINAL] Calling gpt-5.1 for chunk ${idx + 1}`);
          const completion = await openai.chat.completions.create({
            model: FINAL_MODEL,
            messages: [
              {
                role: "system",
                content: [
                  {
                    type: "text",
                    text: `You are a direct and efficient assistant.\n\n${baseSystemPrompt}\n\n${systemInstructions}`,
                    cache_control: { type: "ephemeral" }
                  }
                ] as any
              },
              {
                role: "user",
                content: [
                  {
                    type: "text",
                    text: chunkPrompt,
                    cache_control: { type: "ephemeral" }
                  }
                ] as any
              }
            ],
            response_format: { type: "json_object" },
            temperature: 1.0,
            reasoning_effort: "low"
          });

          const data = JSON.parse(completion.choices[0].message.content || "{}");
          return { idx, slides: data.slides || [], usage: completion.usage };
        });

        const finalResults = await Promise.all(finalPromises);
        finalResults.sort((a, b) => a.idx - b.idx);

        const slides = finalResults.flatMap(r => r.slides);
        console.log(`[FINAL] Generated ${slides.length} main slides from ${finalResults.length} chunks`);

        // Create page objects for main slides
        let allPages = slides.map((slide: any, idx: number) => ({
          page: idx + 1,
          title: slide.title || `ìŠ¬ë¼ì´ë“œ ${idx + 1}`,
          content: slide.content || "",
          keyPoints: slide.keyPoints || []
        }));

        // Generate final 2 pages (review guide) using gpt-5-mini for cost optimization
        console.log(`[REVIEW] Generating final review pages with gpt-5-mini...`);
        const reviewPrompt = type === "exam"
          ? `ë‹¤ìŒì€ ì‹œí—˜ ìë£Œì˜ ì£¼ìš” ìŠ¬ë¼ì´ë“œë“¤ì…ë‹ˆë‹¤:

${slides.map((s: any, i: number) => `[${i + 1}] ${s.title}\n${s.content}\n\ní•µì‹¬ í¬ì¸íŠ¸:\n${s.keyPoints?.map((k: string) => `- ${k}`).join('\n') || ''}`).join('\n\n---\n\n')}

**ì„ë¬´**: ë§ˆì§€ë§‰ 2í˜ì´ì§€ë¥¼ ìƒì„±í•˜ì„¸ìš”.

**page ${slides.length + 1}**: "í•µì‹¬ ê°œë… ì´ì •ë¦¬"
- ì „ì²´ ë‚´ìš©ì˜ í•µì‹¬ ê°œë…ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬
- ì¤‘ìš” ê°œë…ì€ **êµµê²Œ**, í•µì‹¬ ë¬¸ì¥ì€ *ê¸°ìš¸ì„*
- > ì¸ìš©êµ¬ë¡œ ì•”ê¸° íŒ ì¶”ê°€

**page ${slides.length + 2}**: "ì‹œí—˜ ëŒ€ë¹„ ìš”ì•½"
- ì‹œí—˜ì— ê¼­ ë‚˜ì˜¬ ë‚´ìš©ë§Œ ì••ì¶• ì •ë¦¬
- ì˜ˆìƒ ì¶œì œ í¬ì¸íŠ¸ ê°•ì¡°
- ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸ í˜•íƒœ

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{
  "reviewPages": [
    {
      "title": "í•µì‹¬ ê°œë… ì´ì •ë¦¬",
      "content": "ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë‚´ìš©...",
      "keyPoints": ["ì•”ê¸°í•´ì•¼ í•  í•µì‹¬ 1", "ì•”ê¸°í•´ì•¼ í•  í•µì‹¬ 2", "ì•”ê¸°í•´ì•¼ í•  í•µì‹¬ 3"]
    },
    {
      "title": "ì‹œí—˜ ëŒ€ë¹„ ìš”ì•½",
      "content": "ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë‚´ìš©...",
      "keyPoints": ["ì˜ˆìƒ ë¬¸ì œ 1", "ì˜ˆìƒ ë¬¸ì œ 2", "ì˜ˆìƒ ë¬¸ì œ 3"]
    }
  ]
}`
          : `ë‹¤ìŒì€ ì—…ë¬´ ìë£Œì˜ ì£¼ìš” ìŠ¬ë¼ì´ë“œë“¤ì…ë‹ˆë‹¤:

${slides.map((s: any, i: number) => `[${i + 1}] ${s.title}\n${s.content}\n\ní•µì‹¬ ì¸ì‚¬ì´íŠ¸:\n${s.keyPoints?.map((k: string) => `- ${k}`).join('\n') || ''}`).join('\n\n---\n\n')}

**ì„ë¬´**: ë§ˆì§€ë§‰ 2í˜ì´ì§€ë¥¼ ìƒì„±í•˜ì„¸ìš”.

**page ${slides.length + 1}**: "í•µì‹¬ í”„ë¡œì„¸ìŠ¤ ì´ì •ë¦¬"
- ì „ì²´ ì—…ë¬´ íë¦„ê³¼ í•µì‹¬ ë‚´ìš© ì²´ê³„ì  ì •ë¦¬
- ì¤‘ìš” í”„ë¡œì„¸ìŠ¤ëŠ” **êµµê²Œ**, í•µì‹¬ ë¬¸ì¥ì€ *ê¸°ìš¸ì„*
- > ì¸ìš©êµ¬ë¡œ ì‹¤ë¬´ íŒ ì¶”ê°€

**page ${slides.length + 2}**: "ì‹¤ë¬´ ì ìš© ìš”ì•½"
- ì‹¤ë¬´ì— ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ í•µì‹¬ë§Œ ì••ì¶• ì •ë¦¬
- ì²´í¬ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì •ë¦¬

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{
  "reviewPages": [
    {
      "title": "í•µì‹¬ í”„ë¡œì„¸ìŠ¤ ì´ì •ë¦¬",
      "content": "ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë‚´ìš©...",
      "keyPoints": ["ì‹¤ë¬´ í•µì‹¬ 1", "ì‹¤ë¬´ í•µì‹¬ 2", "ì‹¤ë¬´ í•µì‹¬ 3"]
    },
    {
      "title": "ì‹¤ë¬´ ì ìš© ìš”ì•½",
      "content": "ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë‚´ìš©...",
      "keyPoints": ["ì ìš© í¬ì¸íŠ¸ 1", "ì ìš© í¬ì¸íŠ¸ 2", "ì ìš© í¬ì¸íŠ¸ 3"]
    }
  ]
}`;

        const reviewCompletion = await openai.chat.completions.create({
          model: "gpt-5-mini-2025-08-07",
          messages: [
            { role: "system", content: "You are a study guide expert. Generate comprehensive review pages in Korean." },
            { role: "user", content: reviewPrompt }
          ],
          response_format: { type: "json_object" },
          temperature: 1.0
        });

        const reviewData = JSON.parse(reviewCompletion.choices[0].message.content || "{}");
        const reviewPages = reviewData.reviewPages || [];
        console.log(`[REVIEW] Generated ${reviewPages.length} review pages`);

        // Add review pages to allPages
        reviewPages.forEach((page: any, idx: number) => {
          allPages.push({
            page: slides.length + idx + 1,
            title: page.title,
            content: page.content,
            keyPoints: page.keyPoints || []
          });
        });

        // Save to DB
        await supabase
          .from("materials")
          .update({ analysis: { page_analyses: allPages } })
          .eq("id", materialId);

        // Send to frontend
        for (const page of allPages) {
          await sendEvent("page", page);
        }

        // Calculate final cost
        const finalInputCost = finalResults.reduce((sum, r) => sum + (r.usage?.prompt_tokens || 0) * 0.000003, 0);
        const finalOutputCost = finalResults.reduce((sum, r) => sum + (r.usage?.completion_tokens || 0) * 0.000015, 0);
        const finalCost = finalInputCost + finalOutputCost;

        // Calculate review page cost (gpt-5-mini: $0.10/1M input, $0.40/1M output)
        const reviewInputCost = (reviewCompletion.usage?.prompt_tokens || 0) * 0.0000001;
        const reviewOutputCost = (reviewCompletion.usage?.completion_tokens || 0) * 0.0000004;
        const reviewCost = reviewInputCost + reviewOutputCost;

        const totalCost = totalBatchCost + summaryCost + finalCost + reviewCost;

        console.log(`[COST] Final gpt-5.1 call: $${finalCost.toFixed(4)} (input: $${finalInputCost.toFixed(4)}, output: $${finalOutputCost.toFixed(4)})`);
        console.log(`[COST] Review pages (mini): $${reviewCost.toFixed(4)} (input: $${reviewInputCost.toFixed(4)}, output: $${reviewOutputCost.toFixed(4)})`);
        console.log(`[COST] TOTAL: $${totalCost.toFixed(4)} (batch: $${totalBatchCost.toFixed(4)}, summary: $${summaryCost.toFixed(4)}, final: $${finalCost.toFixed(4)}, review: $${reviewCost.toFixed(4)})`);

        // Send completion event
        await sendEvent("complete", { success: true, total_slides: allPages.length });
      }

      await writer.close();
    } catch (error: any) {
      console.error("[STREAM ERROR]", error);
      await sendEvent("error", { error: "Analysis failed", details: error.message });
      await writer.close();
    }
  })();

  return new Response(stream.readable, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache",
      "Connection": "keep-alive",
    },
  });
}
