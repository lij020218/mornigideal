import { NextRequest } from "next/server";
import { auth } from "@/auth";
import { createClient } from "@supabase/supabase-js";
import OpenAI from "openai";
import pdfParse from "pdf-parse-fork";
import crypto from "crypto";
import { extractAndAnalyzeImages, formatImageAnalysis } from "@/lib/pdf-image-extractor";

// Route segment config for large file uploads
export const maxDuration = 300; // 5 minutes for Vercel Pro
export const dynamic = 'force-dynamic';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
});

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!,
  {
    auth: {
      autoRefreshToken: false,
      persistSession: false,
    },
  }
);

// Model Configuration
const MINI_MODEL = "gpt-5-mini-2025-08-07";      // Intelligent chunking
const ADVANCED_MODEL = "gpt-5.1-2025-11-13";    // High-quality conversion

/**
 * SMART HYBRID PIPELINE (2025-12-04)
 *
 * ëª©ì : í’ˆì§ˆ ìœ ì§€ + ë¹„ìš© ì ˆê°
 *
 * Architecture:
 * 1. PDF â†’ Text Extraction (pdf-parse-fork)
 * 2. GPT-5 Mini: ì›ë¬¸ì„ 3-4ê°œ ê· ë“± ì²­í¬ë¡œ ë¶„í•  (ì»¨í…ìŠ¤íŠ¸ ìœ ì§€)
 * 3. GPT-5.1 ìˆœì°¨: ê° ì²­í¬ë¥¼ A ëª¨ë“œë¡œ ë³€í™˜ (ì´ì „ ê²°ê³¼ ì°¸ì¡°)
 *
 * ë¹„ìš© ìµœì í™”:
 * - Mini 1íšŒ: ~$0.001 (ì²­í¬ ë¶„í• )
 * - Advanced 3-4íšŒ: ~$0.03-0.05 (ê° ì²­í¬ 2-3K output)
 * - ì´: ~$0.04-0.06 (ê¸°ì¡´ $0.15 ëŒ€ë¹„ 60-73% ì ˆê°)
 */

export async function POST(request: NextRequest) {
  const encoder = new TextEncoder();
  const stream = new TransformStream();
  const writer = stream.writable.getWriter();

  const sendEvent = async (event: string, data: any) => {
    await writer.write(encoder.encode(`event: ${event}\ndata: ${JSON.stringify(data)}\n\n`));
  };

  (async () => {
    const startTime = Date.now();

    try {
      const session = await auth();
      if (!session?.user?.email) {
        await sendEvent("error", { error: "Unauthorized" });
        await writer.close();
        return;
      }

      const body = await request.json();
      const { blobUrl, fileName: originalFileName, type } = body;

      if (!blobUrl || !originalFileName) {
        await sendEvent("error", { error: "No file URL provided" });
        await writer.close();
        return;
      }

      console.log(`[START] Processing ${originalFileName} from blob: ${blobUrl}`);

      // ====================
      // STEP 1: Download from Blob & Extract Text
      // ====================
      await sendEvent("progress", {
        stage: "download_and_extract",
        message: "íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."
      });

      // Download file from blob storage
      const blobResponse = await fetch(blobUrl);
      if (!blobResponse.ok) {
        throw new Error("Failed to download file from blob storage");
      }

      const buffer = Buffer.from(await blobResponse.arrayBuffer());
      const fileHash = crypto.createHash('sha256').update(buffer).digest('hex');
      const fileName = `${session.user.email}/${fileHash}.pdf`;

      // Upload to Supabase
      let fileUrl = "";
      const { error: uploadError } = await supabase.storage
        .from("materials")
        .upload(fileName, buffer, {
          contentType: "application/pdf",
          upsert: true,
        });

      if (!uploadError) {
        const { data: publicUrlData } = supabase.storage.from("materials").getPublicUrl(fileName);
        fileUrl = publicUrlData.publicUrl;
        console.log(`[STORAGE] Uploaded PDF: ${fileUrl}`);
      } else {
        console.error("[STORAGE] Upload error:", uploadError);
        throw new Error("Failed to upload PDF");
      }

      // Extract text from PDF
      console.log('[PDF-EXTRACT] Extracting text...');
      const pdfData = await pdfParse(buffer);
      let extractedText = pdfData.text;
      const pageCount = pdfData.numpages;

      console.log(`[PDF-PARSE] Extracted ${extractedText.length} chars from ${pageCount} pages`);

      // Extract and analyze images with Google Cloud Vision
      await sendEvent("progress", {
        stage: "image_analysis",
        message: "ì´ë¯¸ì§€ ë¶„ì„ ì¤‘... (Google Cloud Vision)"
      });

      console.log('[IMAGE-EXTRACT] Analyzing images...');
      const apiKey = process.env.GEMINI_API_KEY;
      if (apiKey) {
        const imageAnalyses = await extractAndAnalyzeImages(buffer, apiKey);
        if (imageAnalyses.length > 0) {
          const imageText = formatImageAnalysis(imageAnalyses);
          extractedText = extractedText + imageText;
          console.log(`[IMAGE-EXTRACT] Added ${imageAnalyses.length} image analyses to text`);
        } else {
          console.log('[IMAGE-EXTRACT] No images found or analyzed');
        }
      } else {
        console.warn('[IMAGE-EXTRACT] Skipping - no API key found');
      }

      // ====================
      // STEP 2: GPT-5 Mini - ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í• 
      // ====================
      await sendEvent("progress", {
        stage: "chunking",
        message: "ìµœì  ì²­í¬ë¡œ ë¶„í•  ì¤‘... (GPT-5 Mini)"
      });

      console.log('[GPT-5 Mini] Smart chunking...');

      // Simple character-based chunking (ê· ë“± ë¶„í• )
      const targetChunkSize = Math.ceil(extractedText.length / 3); // 3ë“±ë¶„
      const chunks: { index: number; text: string }[] = [];

      let currentPos = 0;
      let chunkIndex = 0;

      while (currentPos < extractedText.length) {
        const endPos = Math.min(currentPos + targetChunkSize, extractedText.length);
        let actualEndPos = endPos;

        // Try to break at paragraph boundary
        if (actualEndPos < extractedText.length) {
          const nextNewline = extractedText.indexOf('\n\n', endPos - 200);
          if (nextNewline !== -1 && nextNewline < endPos + 200) {
            actualEndPos = nextNewline + 2;
          }
        }

        chunks.push({
          index: chunkIndex,
          text: extractedText.substring(currentPos, actualEndPos).trim()
        });

        currentPos = actualEndPos;
        chunkIndex++;
      }

      console.log(`[CHUNKING] Split into ${chunks.length} chunks (avg ${Math.round(extractedText.length / chunks.length)} chars each)`);

      // ====================
      // STEP 3: Save Material to DB
      // ====================
      console.log('[DB] Creating material record...');

      const material = {
        user_id: session.user.email,
        title: originalFileName.replace(/\.(pdf|txt|doc|docx)$/i, ''),
        content: extractedText.substring(0, 50000),
        type,
        file_url: fileUrl,
        file_hash: fileHash,
        analysis: {
          status: 'processing',
          approach: "smart-hybrid"
        },
        created_at: new Date().toISOString(),
      };

      const { data: insertedMaterial, error: insertError } = await supabase
        .from("materials")
        .insert(material)
        .select()
        .single();

      if (insertError) {
        console.error("[DB] Insert error:", insertError);
        throw insertError;
      }

      console.log(`[DB] Material created with ID: ${insertedMaterial.id}`);

      // ====================
      // STEP 4: GPT-5.1 ìˆœì°¨ ë³€í™˜ (ì»¨í…ìŠ¤íŠ¸ ìœ ì§€)
      // ====================
      await sendEvent("progress", {
        stage: "converting",
        message: "A ëª¨ë“œë¡œ ë³€í™˜ ì¤‘... (GPT-5.1)"
      });

      console.log(`[GPT-5.1 + Mini] Converting and enhancing ${chunks.length} chunks in parallel...`);

      // Parallel conversion AND enhancement with Promise.all
      const conversionPromises = chunks.map(async (chunk, i) => {
        const conversionPrompt = `ë‹¹ì‹ ì€ **ë³€í™˜ê¸°**ì…ë‹ˆë‹¤. ì•„ë˜ ì›ë¬¸ì„ A ëª¨ë“œë¡œ ì••ì¶• ë³€í™˜í•˜ì„¸ìš”.

**í˜„ì¬ ì›ë¬¸** (ì „ì²´ ì¤‘ ${i + 1}/${chunks.length} ë¶€ë¶„):
"""
${chunk.text}
"""

**ë³€í™˜ ê·œì¹™ (A ëª¨ë“œ)**:

1. **ì›ë¬¸ ë¬¸ì¥ ê¸°ì¤€**: ì›ë¬¸ì˜ ë¬¸ì¥ êµ¬ì¡°ì™€ ìˆœì„œ ìœ ì§€
2. **ë¬¸ë‹¨ ë‹¨ìœ„ ì²˜ë¦¬**: ì›ë¬¸ì˜ ë¬¸ë‹¨ ìˆœì„œ ê·¸ëŒ€ë¡œ ìœ ì§€
3. **ì¬í•´ì„Â·ì°½ì‘ ê¸ˆì§€**: ì›ë¬¸ì— ì—†ëŠ” ë‚´ìš© ì ˆëŒ€ ì¶”ê°€ ë¶ˆê°€
4. **í˜•ì‹ ë³€í™˜ë§Œ**:
   - í•µì‹¬ ìš©ì–´: **êµµê²Œ**
   - ê°€ì¥ ì¤‘ìš”í•œ ìš©ì–´: <mark>í•˜ì´ë¼ì´íŠ¸</mark> (ë°˜ë“œì‹œ ì—¬ëŠ” íƒœê·¸ì™€ ë‹«ëŠ” íƒœê·¸ ëª¨ë‘ í¬í•¨)
   - ìˆ˜ì‹: ë°˜ë“œì‹œ LaTeX í˜•ì‹ ($x^2$ ì¸ë¼ì¸ ë˜ëŠ” $$E=mc^2$$ ë¸”ë¡)
   - í‘œ: ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ë³€í™˜
5. **ì–¸ì–´**:
   - ì„¤ëª…ê³¼ ë¬¸ì¥ì€ í•œêµ­ì–´ë¡œ ì‘ì„±
   - ì „ë¬¸ ìš©ì–´, ê³ ìœ ëª…ì‚¬, ê¸°ìˆ  ìš©ì–´ëŠ” ì˜ì–´ ê·¸ëŒ€ë¡œ ìœ ì§€
   - ì˜ˆ: "**Gradient Descent**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤"
6. **ë¶ˆí™•ì‹¤í•˜ë©´ ì›ë¬¸ ì¸ìš©**: í•´ì„í•˜ì§€ ë§ê³  ì›ë¬¸ ê·¸ëŒ€ë¡œ ë³µì‚¬
7. **ì••ì¶• ê°•ë„ (ë§¤ìš° ì¤‘ìš”)**:
   - ëª©í‘œ: ì›ë³¸ ëŒ€ë¹„ 60-65% ë¶„ëŸ‰ìœ¼ë¡œ ì••ì¶•
   - ë°˜ë³µ ì„¤ëª…, ì¥í™©í•œ ì˜ˆì‹œ, ë¶ˆí•„ìš”í•œ ì ‘ì†ì‚¬ ì œê±°
   - í•µì‹¬ë§Œ ë‚¨ê¸°ë˜ ì¤‘ìš” ë‚´ìš©ì€ ì ˆëŒ€ ëˆ„ë½ ê¸ˆì§€
   - ê°™ì€ ë§¥ë½ì˜ ì—¬ëŸ¬ ë¬¸ì¥ì€ í•˜ë‚˜ë¡œ í†µí•©

**ìŠ¬ë¼ì´ë“œ êµ¬ë¶„ (ë§¤ìš° ì¤‘ìš”)**:
- **ìŠ¬ë¼ì´ë“œ ê°œìˆ˜**: ë‚´ìš©ì— ë”°ë¼ ìì—°ìŠ¤ëŸ½ê²Œ ì¡°ì • (ì œí•œ ì—†ìŒ)
- í° ì£¼ì œ/ì„¹ì…˜/ê°œë…ì€ \`## ì œëª©\` í˜•ì‹ìœ¼ë¡œ ì‹œì‘í•˜ì—¬ ê°ê° ë³„ë„ ìŠ¬ë¼ì´ë“œë¡œ êµ¬ì„±
- **ë‚´ìš©ì´ ë§ì€ ê²½ìš°**: ì£¼ì œë³„ë¡œ ì ì ˆíˆ ë¶„í• í•˜ì—¬ ê°ê° ìŠ¬ë¼ì´ë“œë¡œ ìƒì„±
- **ë‚´ìš©ì´ ì ì€ ê²½ìš°**: ê´€ë ¨ ê°œë…ì„ í†µí•©í•˜ì—¬ ì ì ˆí•œ ê°œìˆ˜ë¡œ ì¡°ì •
- ê° ìŠ¬ë¼ì´ë“œëŠ” í•˜ë‚˜ì˜ ì™„ê²°ëœ ì£¼ì œë‚˜ ê°œë…ì„ ë‹´ì„ ê²ƒ
- ì‘ì€ ì„¸ë¶€ ì£¼ì œë“¤ì€ ### ì†Œì œëª©ìœ¼ë¡œ í•˜ë‚˜ì˜ ìŠ¬ë¼ì´ë“œ ì•ˆì—ì„œ êµ¬ë¶„
- **ì ˆëŒ€ ê¸ˆì§€**: ë¹ˆ ìŠ¬ë¼ì´ë“œë‚˜ ì˜ë¯¸ì—†ëŠ” ìŠ¬ë¼ì´ë“œë¥¼ ë§Œë“¤ì§€ ë§ ê²ƒ
- **ì¤‘ìš”**: ì›ë³¸ì— ì‹¤ì œë¡œ ìˆëŠ” ë‚´ìš©ë§Œ ìŠ¬ë¼ì´ë“œë¡œ ë§Œë“¤ ê²ƒ

**ìˆ˜ì‹ ë³€í™˜ (ë§¤ìš° ì¤‘ìš”)**:
- ëª¨ë“  ìˆ˜ì‹ì€ LaTeX ë¬¸ë²•ìœ¼ë¡œ ë³€í™˜: $f(x) = x^2$, $$\\int_0^1 x dx$$
- ë¶„ìˆ˜: $\\frac{a}{b}$, ì œê³±: $x^2$, ì•„ë˜ì²¨ì: $x_i$
- ê·¸ë¦¬ìŠ¤ ë¬¸ì: $\\alpha, \\beta, \\mu, \\sigma$
- í•©: $\\sum_{i=1}^n$, ì ë¶„: $\\int_a^b$
- í–‰ë ¬: $$\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$$
- **ì ˆëŒ€ ê¸ˆì§€**: ìˆ˜ì‹ì„ ì¼ë°˜ í…ìŠ¤íŠ¸ë‚˜ ë°±í‹±ìœ¼ë¡œ ê°ì‹¸ê¸°

**í‘œ ë³€í™˜ ê·œì¹™ (ë§¤ìš° ë§¤ìš° ì¤‘ìš” - ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ!)**:

ì›ë¬¸ì— í‘œ í˜•íƒœì˜ ë°ì´í„°ê°€ ìˆìœ¼ë©´ **ë¬´ì¡°ê±´** ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ë³€í™˜:

**ì˜¬ë°”ë¥¸ ì˜ˆì‹œ**:

| Support | Frequency | Proportion |
| :--- | :--- | :--- |
| ì°¨ì€ìš° | 27 | 0.54 |
| ë°•ë³´ê²€ | 23 | 0.46 |

**ë˜ ë‹¤ë¥¸ ì˜ˆì‹œ**:

| í•­ëª© | ê°’ | ë¹„ê³  |
| :--- | :--- | :--- |
| í‰ê·  | 85.3 | ìƒìœ„ 10% |
| í‘œì¤€í¸ì°¨ | 12.7 | - |

**í•„ìˆ˜ ê·œì¹™ (ì ˆëŒ€ ì§€ì¼œì•¼ í•¨!)**:
1. í‘œ ìœ„ì•„ë˜ë¡œ ë°˜ë“œì‹œ **ë¹ˆ ì¤„ 2ê°œ** (ì¤„ë°”ê¿ˆ 2ë²ˆ)
2. **ì²« ë²ˆì§¸ í–‰**: í—¤ë” (| í—¤ë”1 | í—¤ë”2 |)
3. **ë‘ ë²ˆì§¸ í–‰**: ì •ë ¬ êµ¬ë¶„ì„  (| :--- | :--- |) - **ë°˜ë“œì‹œ ì½œë¡ (:) í¬í•¨í•˜ì—¬ ì •ë ¬ ëª…ì‹œ**
4. **ì„¸ ë²ˆì§¸ í–‰ë¶€í„°**: ë°ì´í„° í–‰
5. ê° ì…€ì€ **ë°˜ë“œì‹œ** | ê¸°í˜¸ë¡œ êµ¬ë¶„
6. ëª¨ë“  í–‰ì˜ ì—´ ê°œìˆ˜ **ë™ì¼**í•˜ê²Œ ìœ ì§€
7. **ì ˆëŒ€ ê¸ˆì§€**: í‘œë¥¼ "í•­ëª©1: ê°’1, í•­ëª©2: ê°’2" ê°™ì€ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
8. **ì ˆëŒ€ ê¸ˆì§€**: í‘œë¥¼ ë¦¬ìŠ¤íŠ¸ë‚˜ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

**ì ˆëŒ€ ê¸ˆì§€**:
- âŒ ì›ë¬¸ì— ì—†ëŠ” ì„¤ëª… ì¶”ê°€
- âŒ "ê°œë…:", "ì •ì˜:" ê°™ì€ í…œí”Œë¦¿
- âŒ ì™¸ë¶€ ì§€ì‹ìœ¼ë¡œ ë³´ì¶©
- âŒ í‘œë¥¼ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
- âŒ ìˆ˜ì‹ì„ ë°±í‹±(\`)ì´ë‚˜ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì‘ì„±
- âŒ <mark> íƒœê·¸ë¥¼ ë‹«ì§€ ì•Šê³  </mark>ë§Œ ì‘ì„±
- âŒ <mark>ì™€ ** ë³¼ë“œë¥¼ í•¨ê»˜ ì‚¬ìš© (ì˜ˆ: <mark>**í…ìŠ¤íŠ¸**</mark>) - ì¤‘ìš” í‘œì‹œëŠ” <mark>ë§Œ ì‚¬ìš©í•  ê²ƒ

**ì¶œë ¥**: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ (## ì œëª© í¬í•¨, ì›ë¬¸ ìˆœì„œ ìœ ì§€)`;

        // GPT-5.1: Convert to A mode
        const conversionResponse = await openai.chat.completions.create({
          model: ADVANCED_MODEL,
          messages: [
            {
              role: "system",
              content: "ë‹¹ì‹ ì€ ì •í™•í•œ ë³€í™˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê·œì¹™ì„ ì •í™•íˆ ë”°ë¦…ë‹ˆë‹¤. íŠ¹íˆ í‘œëŠ” ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
            },
            {
              role: "user",
              content: conversionPrompt
            }
          ],
          temperature: 1.0,
          reasoning_effort: "low",
        });

        const converted = conversionResponse.choices[0].message.content!.trim();
        console.log(`[GPT-5.1] Chunk ${i + 1} converted: ${converted.length} chars`);

        // Now enhance with Mini model
        const enhancementPrompt = `ë‹¹ì‹ ì€ ì‹œí—˜ ì¤€ë¹„ë¥¼ ë•ëŠ” í•™ìŠµ ì½”ì¹˜ì…ë‹ˆë‹¤.

ì•„ë˜ í•™ìŠµ ìë£Œë¥¼ ë¶„ì„í•˜ê³ , ê° ## ì œëª© ì„¹ì…˜ ë§ˆì§€ë§‰ì— **"### ğŸ’¡ í•œê±¸ìŒ ë”!"** ì„¹ì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”.

**ì¤‘ìš” ê·œì¹™**:
1. ì›ë³¸ ë‚´ìš©ì€ ì ˆëŒ€ ìˆ˜ì •í•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ìœ ì§€
2. ê° ## ì œëª© ì„¹ì…˜ ëì—ë§Œ "### ğŸ’¡ í•œê±¸ìŒ ë”!" ì¶”ê°€
3. "í•œê±¸ìŒ ë”!" ë‚´ìš©: ì‹œí—˜ íŒ, ì‹¤ì „ ì‘ìš©, ì•”ê¸° ìš”ë ¹, ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜ ë“±
4. 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±
5. **ë°˜ë“œì‹œ ì›ë³¸ ìë£Œì—ì„œ ìœ ë˜í•œ êµ¬ì²´ì ì¸ íŒë§Œ ì‘ì„±**

**ì ˆëŒ€ ê¸ˆì§€**:
- âŒ "ê°•ì˜ ë…¸íŠ¸ì™€ ì—°ë™í•´ í™•ì¸í•˜ì„¸ìš”" ê°™ì€ ì¼ë°˜ì ì¸ ì¡°ì–¸
- âŒ "êµì¬ì™€ ëŒ€ì¡°í•©ë‹ˆë‹¤" ê°™ì€ í•™ìŠµ ë°©ë²•ë¡ 
- âŒ "ì˜¤íƒ€ë‚˜ ë¶€ì¡±í•œ ë¶€ë¶„ì´ ìˆìœ¼ë©´" ê°™ì€ ë©”íƒ€ ì¡°ì–¸
- âŒ ì›ë³¸ ìë£Œì™€ ë¬´ê´€í•œ ì¼ë°˜ë¡ 
- âŒ ë¹ˆ ê³µê°„ì„ ì±„ìš°ê¸° ìœ„í•œ ì˜ë¯¸ì—†ëŠ” í…ìŠ¤íŠ¸

**í•™ìŠµ ìë£Œ**:
${converted}`;

        const miniResponse = await openai.chat.completions.create({
          model: MINI_MODEL,
          messages: [{ role: "user", content: enhancementPrompt }],
          temperature: 1.0,
        });

        let enhanced = miniResponse.choices[0].message.content!.trim();

        // ë°±í‹± ì½”ë“œ ë¸”ë¡ ì œê±°
        enhanced = enhanced
          .replace(/^```[a-z]*\n/gm, '')
          .replace(/\n```$/gm, '')
          .trim();

        console.log(`[Mini] Chunk ${i + 1} enhanced: ${enhanced.length} chars`);

        return { index: i, content: enhanced };
      });

      // Wait for all conversions AND enhancements to complete
      const convertedChunks = await Promise.all(conversionPromises);

      // Sort by index and join
      convertedChunks.sort((a, b) => a.index - b.index);
      let fullContent = convertedChunks.map(c => c.content).join("\n\n");

      console.log(`[PARALLEL] All chunks converted and enhanced: ${fullContent.length} total chars`);

      // Stream all content at once after parallel completion
      for (let i = 0; i < convertedChunks.length; i++) {
        await sendEvent("content", {
          content: convertedChunks[i].content,
          isComplete: false,
          progress: ((i + 1) / convertedChunks.length) * 100
        });
      }

      // Note: "í•œê±¸ìŒ ë”!" ì„¹ì…˜ì€ ê° ì²­í¬ ë³€í™˜ ì‹œ ì´ë¯¸ ë³‘ë ¬ë¡œ ì¶”ê°€ë¨
      // Note: í•µì‹¬ ê°œë…ì€ ì‚¬ìš©ìê°€ "í•µì‹¬ ê°œë…" ë²„íŠ¼ í´ë¦­ ì‹œ on-demandë¡œ ìƒì„±ë©ë‹ˆë‹¤.

      // ====================
      // STEP 5: Update DB with final content
      // ====================
      const totalDuration = Date.now() - startTime;
      const estimatedInputTokens = chunks.reduce((sum, c) => sum + Math.ceil(c.text.length / 4), 0);
      const estimatedOutputTokens = Math.ceil(fullContent.length / 4);
      const estimatedCost =
        (estimatedInputTokens / 1_000_000) * 2.50 +
        (estimatedOutputTokens / 1_000_000) * 10.00;

      const metricsSummary = {
        totalDuration,
        approach: "smart-hybrid",
        apiCalls: chunks.length,
        chunkCount: chunks.length,
        outputLength: fullContent.length,
        estimatedInputTokens,
        estimatedOutputTokens,
        estimatedCost,
        pdfPages: pageCount,
        extractedTextLength: extractedText.length,
      };

      await supabase
        .from("materials")
        .update({
          analysis: {
            content: fullContent,
            metrics: metricsSummary,
            status: 'completed',
            generatedAt: new Date().toISOString(),
          }
        })
        .eq("id", insertedMaterial.id);

      console.log(`[METRICS] Complete: ${(totalDuration / 1000).toFixed(2)}s, ~$${estimatedCost.toFixed(4)}`);

      await sendEvent("complete", {
        materialId: insertedMaterial.id,
        content: fullContent,
        metrics: metricsSummary,
      });

      await writer.close();
    } catch (error: any) {
      console.error("[ERROR]", error);
      await sendEvent("error", { error: error.message });
      await writer.close();
    }
  })();

  return new Response(stream.readable, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache",
      "Connection": "keep-alive",
    },
  });
}
