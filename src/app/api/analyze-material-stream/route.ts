import { NextRequest } from "next/server";
import { auth } from "@/auth";
import { createClient } from "@supabase/supabase-js";
import OpenAI from "openai";
import pdfParse from "pdf-parse-fork";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
});

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!,
  {
    auth: {
      autoRefreshToken: false,
      persistSession: false,
    },
  }
);

const PAGES_PER_BATCH = 5;
const CHUNK_MODEL = "gpt-5-mini-2025-08-07";
const FINAL_MODEL = "gpt-5.1-2025-11-13";
const EMBEDDING_MODEL = "text-embedding-3-small";
const SIMILARITY_THRESHOLD = 0.68;

const STUDY_SYSTEM_PROMPT = `ë‹¹ì‹ ì€ MIT, Stanfordê¸‰ ì„¸ê³„ ìµœê³  ëŒ€í•™ì˜ ì €ëª…í•œ êµìˆ˜ì…ë‹ˆë‹¤. í•™ìƒë“¤ì´ ê¹Šì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ëª…ë£Œí•˜ê³  í†µì°°ë ¥ ìˆê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.

**ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­**:
- "ì„¤ëª… â†’ ì—°ê²°:", "í•µì‹¬ê°œë…:", "ì‹œí—˜í¬ì¸íŠ¸:", "ì¤‘ìš”:" ê°™ì€ ë”±ë”±í•œ ë¼ë²¨ ì‚¬ìš© ê¸ˆì§€
- ë‹¨ìˆœ ìš”ì•½ì´ë‚˜ ëª©ë¡ ë‚˜ì—´ ê¸ˆì§€
- í”¼ìƒì ì´ê±°ë‚˜ êµê³¼ì„œì ì¸ ì„¤ëª… ê¸ˆì§€

**í•„ìˆ˜ ì‘ì„± ë°©ì‹**:
- ê°œë…ì˜ ë³¸ì§ˆê³¼ ë§¥ë½ì„ ëª…ë£Œí•˜ê²Œ ì„¤ëª…
- "í•µì‹¬ì€ ~ì´ë‹¤", "ì™œ ì´ê²ƒì´ ì¤‘ìš”í•œê°€", "ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ”" ê°™ì€ ì „ë¬¸ì ì´ë©´ì„œë„ ëª…í™•í•œ í‘œí˜„ ì‚¬ìš©
- ê°œë… â†’ ì›ë¦¬ â†’ ì ìš© â†’ í•¨ì˜(implications) ìˆœì„œë¡œ ë…¼ë¦¬ì  íë¦„ êµ¬ì„±
- í•œ ë¬¸ë‹¨ì´ ë‹¤ìŒ ë¬¸ë‹¨ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ë˜, ê° ë¬¸ë‹¨ì€ ëª…í™•í•œ í†µì°° ì œê³µ
- ì¶”ìƒì  ê°œë…ì„ êµ¬ì²´ì  ì‚¬ë¡€ë¡œ ëª…í™•íˆ ì„¤ëª…

í•­ìƒ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ëª…ë£Œí•¨ê³¼ ê¹Šì´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.`;

const WORK_SYSTEM_PROMPT = `ë‹¹ì‹ ì€ í•´ë‹¹ ë¶„ì•¼ì—ì„œ 10ë…„+ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í›„ë°°ë“¤ì´ ê¹Šì´ ì´í•´í•˜ê³  ì‹¤ë¬´ì— ì ìš©í•  ìˆ˜ ìˆë„ë¡ ëª…í™•í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.

**ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­**:
- "í•µì‹¬ê°œë…:", "ì—…ë¬´í¬ì¸íŠ¸:", "ì¤‘ìš”:" ê°™ì€ ë”±ë”±í•œ ë¼ë²¨ ì‚¬ìš© ê¸ˆì§€
- í˜•ì‹ì ì¸ ë³´ê³ ì„œì²´ë‚˜ ê´€ë£Œì  í‘œí˜„ ê¸ˆì§€
- í”¼ìƒì ì´ê±°ë‚˜ ë§¤ë‰´ì–¼ì‹ ì„¤ëª… ê¸ˆì§€

**í•„ìˆ˜ ì‘ì„± ë°©ì‹**:
- ì—…ë¬´ì˜ ë³¸ì§ˆê³¼ ë§¥ë½ì„ ëª…í™•í•˜ê²Œ ì„¤ëª…
- "í•µì‹¬ì€ ~ì´ë‹¤", "ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ì ì€", "ì‹¤ë¬´ì ìœ¼ë¡œ ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ”" ê°™ì€ ì „ë¬¸ì ì´ë©´ì„œë„ ëª…í™•í•œ í‘œí˜„ ì‚¬ìš©
- ë°°ê²½ â†’ í•µì‹¬ ì›ë¦¬ â†’ ì‹¤ì œ ì ìš© â†’ ì£¼ì˜ì‚¬í•­ ìˆœì„œë¡œ ë…¼ë¦¬ì  íë¦„ êµ¬ì„±
- í•œ ë¬¸ë‹¨ì´ ë‹¤ìŒ ë¬¸ë‹¨ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ë˜, ê° ë¬¸ë‹¨ì€ ëª…í™•í•œ í†µì°° ì œê³µ
- ì¶”ìƒì  í”„ë¡œì„¸ìŠ¤ë¥¼ êµ¬ì²´ì  ìƒí™©ìœ¼ë¡œ ëª…í™•íˆ ì„¤ëª…

í•­ìƒ ì‹œë‹ˆì–´ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ëª…ë£Œí•¨ê³¼ ê¹Šì´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.`;

// Cosine similarity helper
function cosineSimilarity(a: number[], b: number[]): number {
  let dotProduct = 0;
  let normA = 0;
  let normB = 0;

  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i];
    normA += a[i] * a[i];
    normB += b[i] * b[i];
  }

  return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
}

// OPTIMIZED: Full parallel clustering
async function groupSimilarChunksParallel(chunks: any[]): Promise<any[][]> {
  if (chunks.length <= 3) return [chunks];

  console.log(`[EMBEDDING] Creating embeddings for ${chunks.length} chunks (FULL PARALLEL)`);

  // Create ALL embeddings in parallel (no batching)
  const embeddingPromises = chunks.map(async (chunk) => {
    const text = `${chunk.title}\n${chunk.content}`;
    const response = await openai.embeddings.create({
      model: EMBEDDING_MODEL,
      input: text,
    });
    return {
      chunk,
      embedding: response.data[0].embedding,
    };
  });

  const chunksWithEmbeddings = await Promise.all(embeddingPromises);
  console.log(`[EMBEDDING] All ${chunks.length} embeddings created in parallel`);

  // Clustering
  const groups: any[][] = [];
  const used = new Set<number>();

  for (let i = 0; i < chunksWithEmbeddings.length; i++) {
    if (used.has(i)) continue;

    const group = [chunksWithEmbeddings[i].chunk];
    used.add(i);

    for (let j = i + 1; j < chunksWithEmbeddings.length; j++) {
      if (used.has(j)) continue;

      const similarity = cosineSimilarity(
        chunksWithEmbeddings[i].embedding,
        chunksWithEmbeddings[j].embedding
      );

      if (similarity > SIMILARITY_THRESHOLD) {
        group.push(chunksWithEmbeddings[j].chunk);
        used.add(j);
      }
    }

    groups.push(group);
  }

  console.log(`[CLUSTERING] Grouped ${chunks.length} chunks into ${groups.length} clusters`);
  return groups;
}

// OPTIMIZED: Compress all clusters in parallel
async function compressClustersParallel(groups: any[][], type: string): Promise<string[]> {
  console.log(`[COMPRESSION] Compressing ${groups.length} clusters (FULL PARALLEL)`);

  const compressionPromises = groups.map(async (group, idx) => {
    const allContents = group.map((c, i) =>
      `[Chunk ${i + 1}] ${c.title}:\n${c.content}`
    ).join("\n\n");

    const compressionPrompt = type === "exam"
      ? `ë‹¤ìŒì€ ê°™ì€ ì£¼ì œë¡œ ë¬¶ì¸ ${group.length}ê°œì˜ í•™ìŠµ ë‚´ìš©ì…ë‹ˆë‹¤:\n\n${allContents}\n\n**ì„ë¬´**: ì´ ë‚´ìš©ë“¤ì„ ê°•ì˜ì‹¤ì—ì„œ ì„¤ëª…í•˜ë“¯ì´ ìì—°ìŠ¤ëŸ½ê²Œ í•˜ë‚˜ì˜ ê¸´ ì„¤ëª…ìœ¼ë¡œ í†µí•©í•˜ì„¸ìš”.\n\në‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:\n{\n  "topic": "ì£¼ì œ ì œëª©",\n  "summary": "ê°•ì˜ì‹¤ì—ì„œ ì„¤ëª…í•˜ë“¯ì´ ìì—°ìŠ¤ëŸ½ê²Œ íë¥´ëŠ” ê¸´ í…ìŠ¤íŠ¸..."\n}`
      : `ë‹¤ìŒì€ ê°™ì€ ì£¼ì œë¡œ ë¬¶ì¸ ${group.length}ê°œì˜ ì—…ë¬´ ë‚´ìš©ì…ë‹ˆë‹¤:\n\n${allContents}\n\n**ì„ë¬´**: ì´ ë‚´ìš©ë“¤ì„ ë™ë£Œì—ê²Œ ì„¤ëª…í•˜ë“¯ì´ ìì—°ìŠ¤ëŸ½ê²Œ í•˜ë‚˜ì˜ ê¸´ ì„¤ëª…ìœ¼ë¡œ í†µí•©í•˜ì„¸ìš”.\n\në‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:\n{\n  "topic": "ì£¼ì œ ì œëª©",\n  "summary": "ë™ë£Œì—ê²Œ ì„¤ëª…í•˜ë“¯ì´ ìì—°ìŠ¤ëŸ½ê²Œ íë¥´ëŠ” ê¸´ í…ìŠ¤íŠ¸..."\n}`;

    const compression = await openai.chat.completions.create({
      model: CHUNK_MODEL,
      messages: [
        { role: "system", content: type === "exam" ? STUDY_SYSTEM_PROMPT : WORK_SYSTEM_PROMPT },
        { role: "user", content: compressionPrompt },
      ],
      response_format: { type: "json_object" },
      temperature: 1.0,
    });

    const result = JSON.parse(compression.choices[0].message.content || "{}");
    return {
      idx: idx + 1,
      summary: `### Topic ${idx + 1}: ${result.topic}\n\n${result.summary}`
    };
  });

  const results = await Promise.all(compressionPromises);
  results.sort((a, b) => a.idx - b.idx);

  console.log(`[COMPRESSION] All ${groups.length} clusters compressed`);
  return results.map(r => r.summary);
}

export async function POST(request: NextRequest) {
  const encoder = new TextEncoder();
  const stream = new TransformStream();
  const writer = stream.writable.getWriter();

  const sendEvent = async (event: string, data: any) => {
    await writer.write(encoder.encode(`event: ${event}\ndata: ${JSON.stringify(data)}\n\n`));
  };

  (async () => {
    try {
      const session = await auth();
      if (!session?.user?.email) {
        await sendEvent("error", { error: "Unauthorized" });
        await writer.close();
        return;
      }

      const formData = await request.formData();
      const file = formData.get("file") as File;
      const type = formData.get("type") as string;

      if (!file || !type || !["exam", "work"].includes(type)) {
        await sendEvent("error", { error: "Invalid parameters" });
        await writer.close();
        return;
      }

      const isPDF = file.type === "application/pdf";
      let fileUrl: string | null = null;
      let materialId: string | null = null;

      await sendEvent("progress", { stage: "upload", message: "íŒŒì¼ ì—…ë¡œë“œ ì¤‘..." });

      if (isPDF) {
        const arrayBuffer = await file.arrayBuffer();
        const buffer = Buffer.from(arrayBuffer);

        // Upload PDF
        const sanitizedEmail = (session.user.email || '').replace(/[^a-zA-Z0-9]/g, '_');
        const sanitizedFileName = file.name.replace(/\s+/g, '_').replace(/[^\w.-]/g, '');
        const fileName = `${Date.now()}_${sanitizedEmail}_${sanitizedFileName}`;

        const { data: uploadData, error: uploadError } = await supabase.storage
          .from("materials")
          .upload(fileName, buffer, { contentType: "application/pdf", upsert: false });

        if (!uploadError) {
          const { data: publicUrlData } = supabase.storage.from("materials").getPublicUrl(fileName);
          fileUrl = publicUrlData.publicUrl;
        }

        // Extract text
        await sendEvent("progress", { stage: "extract", message: "PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..." });
        const pdfData = await pdfParse(buffer);
        const fullContent = pdfData.text;
        const totalPages = pdfData.numpages;

        // Create material record first (so we can stream updates to it)
        const { data: material, error: insertError } = await supabase
          .from("materials")
          .insert({
            user_id: session.user.email,
            title: file.name,
            content: fullContent.substring(0, 50000),
            type: type,
            file_url: fileUrl,
            analysis: { page_analyses: [] }
          })
          .select()
          .single();

        if (insertError) {
          await sendEvent("error", { error: "Database error", details: insertError.message });
          await writer.close();
          return;
        }

        materialId = material.id;
        await sendEvent("material_created", { id: materialId });

        // Split into pages
        const avgCharsPerPage = Math.ceil(fullContent.length / totalPages);
        const pages: Array<{ pageNum: number; text: string }> = [];
        for (let i = 0; i < totalPages; i++) {
          const start = i * avgCharsPerPage;
          const end = Math.min((i + 1) * avgCharsPerPage, fullContent.length);
          pages.push({ pageNum: i + 1, text: fullContent.substring(start, end) });
        }

        // Process pages in batches IN PARALLEL
        const batches: Array<Array<{ pageNum: number; text: string }>> = [];
        for (let i = 0; i < pages.length; i += PAGES_PER_BATCH) {
          batches.push(pages.slice(i, i + PAGES_PER_BATCH));
        }

        await sendEvent("progress", { stage: "analyze_chunks", message: `${batches.length}ê°œ ë°°ì¹˜ ë¶„ì„ ì¤‘...`, total: batches.length });

        const batchPromises = batches.map(async (batch, batchIdx) => {
          const batchText = batch.map((p) => `=== í˜ì´ì§€ ${p.pageNum} ===\n${p.text}`).join("\n\n");
          const prompt = type === "exam"
            ? `ë‹¹ì‹ ì€ MIT êµìˆ˜ì…ë‹ˆë‹¤. ë‹¤ìŒ ê°•ì˜ í˜ì´ì§€ë“¤ì„ ë¶„ì„í•˜ì—¬ í•™ìƒë“¤ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì„¸ìš”.

${batchText}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{
  "title": "ì´ ì„¹ì…˜ì˜ ëª…í™•í•œ ì œëª©",
  "content": "ê°•ì˜ì‹¤ì—ì„œ ì„¤ëª…í•˜ë“¯ì´ ìì—°ìŠ¤ëŸ½ê²Œ íë¥´ëŠ” ê¸´ í…ìŠ¤íŠ¸. ê°œë… ì •ì˜, ìˆ˜ì‹, ì˜ˆì œë¥¼ í¬í•¨í•˜ì—¬ ì™„ì „íˆ ì„¤ëª…."
}`
            : `ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ í›„ë°°ë“¤ì´ ì‹¤ë¬´ì— ì ìš©í•  ìˆ˜ ìˆë„ë¡ í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì„¸ìš”.

${batchText}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{
  "title": "ì´ ì„¹ì…˜ì˜ ëª…í™•í•œ ì œëª©",
  "content": "ë™ë£Œì—ê²Œ ì„¤ëª…í•˜ë“¯ì´ ìì—°ìŠ¤ëŸ½ê²Œ íë¥´ëŠ” ê¸´ í…ìŠ¤íŠ¸. ê°œë…, í”„ë¡œì„¸ìŠ¤, ì ìš© ë°©ë²•ì„ í¬í•¨í•˜ì—¬ ì™„ì „íˆ ì„¤ëª…."
}`;

          const completion = await openai.chat.completions.create({
            model: CHUNK_MODEL,
            messages: [
              { role: "system", content: type === "exam" ? STUDY_SYSTEM_PROMPT : WORK_SYSTEM_PROMPT },
              { role: "user", content: prompt },
            ],
            response_format: { type: "json_object" },
            temperature: 1.0,
          });

          console.log(`[BATCH ${batchIdx}] Usage:`, completion.usage);

          const result = JSON.parse(completion.choices[0].message.content || "{}");
          return { batchIdx, summary: result, usage: completion.usage };
        });

        const batchResults = await Promise.all(batchPromises);
        batchResults.sort((a, b) => a.batchIdx - b.batchIdx);
        const chunkSummaries = batchResults.map(r => r.summary);

        // Calculate total batch cost
        const totalBatchCost = batchResults.reduce((sum, batch) => {
          const inputCost = (batch.usage?.prompt_tokens || 0) * 0.0000001; // $0.10 per 1M tokens for gpt-5-mini input
          const outputCost = (batch.usage?.completion_tokens || 0) * 0.0000004; // $0.40 per 1M tokens for gpt-5-mini output
          return sum + inputCost + outputCost;
        }, 0);
        console.log(`[COST] All batches: $${totalBatchCost.toFixed(4)}`);

        const summaryCost = 0; // No summary step needed

        // CLUSTERING: Group similar chunks using embeddings
        await sendEvent("progress", { stage: "clustering", message: "ìœ ì‚¬ ì„¹ì…˜ ê·¸ë£¹í™” ì¤‘..." });
        const clusteredGroups = await groupSimilarChunksParallel(chunkSummaries);
        console.log(`[CLUSTERING] Created ${clusteredGroups.length} groups from ${chunkSummaries.length} chunks`);

        // COMPRESS: Compress each cluster in parallel (using mini)
        await sendEvent("progress", { stage: "compress", message: "í´ëŸ¬ìŠ¤í„° ì••ì¶• ì¤‘..." });
        const compressedSummaries = await compressClustersParallel(clusteredGroups, type);
        console.log(`[COMPRESS] Compressed ${clusteredGroups.length} clusters into ${compressedSummaries.length} summaries`);

        // FINAL INTEGRATION: Single gpt-5.1 call to generate all slides
        await sendEvent("progress", { stage: "final_integration", message: "ìµœì¢… ìŠ¬ë¼ì´ë“œ ìƒì„± ì¤‘..." });

        const finalPrompt = type === "exam"
          ? `ë‹¹ì‹ ì€ ëŒ€í•™ ì‹œí—˜ ëŒ€ë¹„ ì „ë¬¸ íŠœí„°ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ê°•ì˜ ìë£Œë¥¼ ì£¼ì œë³„ë¡œ ì••ì¶•í•œ ${compressedSummaries.length}ê°œì˜ Topic ìš”ì•½ì…ë‹ˆë‹¤.

${compressedSummaries.join('\n\n====================\n\n')}

**ì„ë¬´**: ì´ Topic ìš”ì•½ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ëŒ€í•™ìƒìš© ìµœì¢… í•™ìŠµ ìŠ¬ë¼ì´ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.

**ëª©í‘œ**:
- ì „ì²´ ë‚´ìš©ì„ 12-18ê°œ ìŠ¬ë¼ì´ë“œë¡œ ì••ì¶• (ë§ˆì§€ë§‰ 2í˜ì´ì§€ëŠ” ì „ì²´ ìš”ì•½)
- ê° ìŠ¬ë¼ì´ë“œëŠ” í•˜ë‚˜ì˜ í•µì‹¬ ì£¼ì œë¥¼ ë‹¤ë£¸
- ì‹œí—˜ì— ë‚˜ì˜¬ ë§Œí•œ í•µì‹¬ ë‚´ìš©ë§Œ í¬í•¨
- Topicê°„ ë…¼ë¦¬ì  íë¦„ ìœ ì§€

**content ì‘ì„± ê·œì¹™** (ë§¤ìš° ì¤‘ìš”!):
1. **ì™„ê²°ëœ ë¬¸ì¥/ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±** - ì ˆëŒ€ "ë¡œ, ì—¬ê¸°ì„œ", "ì€ ~", "ëŠ” ~" ê°™ì´ ì•ë’¤ê°€ ì˜ë¦° ë¬¸ì¥ì„ ë§Œë“¤ì§€ ë§ˆë¼
2. **ìˆ˜ì‹ê³¼ ë³€ìˆ˜ ì„¤ëª…ì€ í•œ ë¬¸ë‹¨ì— í•¨ê»˜** - ìˆ˜ì‹ì„ ì“´ ë‹¤ìŒ, ê°™ì€ ë¬¸ë‹¨ ì•ˆì—ì„œ ëª¨ë“  ë³€ìˆ˜ë¥¼ ì„¤ëª…
3. **Markdown í˜•ì‹**: ë¬¸ë‹¨ êµ¬ë¶„ì„ ìœ„í•´ ë¹ˆ ì¤„(\\n\\n) ì‚¬ìš©
4. **ì¤‘ìš”í•œ ê°œë…/ìš©ì–´ëŠ” **êµµê²Œ**** (ë³´ë¼ìƒ‰), **í•µì‹¬ ë¬¸ì¥ì€ *ê¸°ìš¸ì„*** (íŒŒë€ìƒ‰)
5. **ë³€ìˆ˜/ìˆ˜ì‹ì€ LaTeX**: $ë³€ìˆ˜$, $$ê³µì‹$$ (ì£¼í™©ìƒ‰ìœ¼ë¡œ í‘œì‹œë¨)
6. **ì¶”ê°€ ì„¤ëª…ì€ > ì¸ìš©êµ¬** (ì‹œì•ˆ ë¸”ë£¨ ì¹´ë“œ)

**ì˜¬ë°”ë¥¸ ì˜ˆì‹œ**:
"**IDF (Inverse Document Frequency)**ëŠ” ë‹¨ì–´ì˜ í¬ì†Œì„±ì„ ì¸¡ì •í•œë‹¤.\\n\\nê³µì‹ì€ $$IDF(t) = \\\\log(N / (1 + n_t))$$ì´ë‹¤. ì—¬ê¸°ì„œ \`N\`ì€ ì „ì²´ ë¬¸ì„œ ìˆ˜, \`n_t\`ëŠ” ë‹¨ì–´ tê°€ ë“±ì¥í•œ ë¬¸ì„œ ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. *í”í•œ ë‹¨ì–´ì¼ìˆ˜ë¡ IDF ê°’ì´ ë‚®ì•„ì§„ë‹¤.*\\n\\n> ğŸ’¡ **í•œ ê±¸ìŒ ë”**: ëª¨ë“  ë¬¸ì„œì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´(n_t=N)ëŠ” IDFê°€ 0ì— ê°€ê¹Œì›Œì ¸ ì¤‘ìš”ë„ê°€ ë‚®ì•„ì§„ë‹¤. ì´ê²ƒì´ 'the', 'a' ê°™ì€ ë¶ˆìš©ì–´ê°€ ìë™ìœ¼ë¡œ í•„í„°ë§ë˜ëŠ” ì›ë¦¬ë‹¤."

**ì˜ëª»ëœ ì˜ˆì‹œ (ì ˆëŒ€ í•˜ì§€ ë§ˆë¼)**:
"[ì½”ë“œë¸”ë¡]IDF(t) = log(N / (1 + n_t))[ì½”ë“œë¸”ë¡]\\n\\në¡œ, ì—¬ê¸°ì„œ\\n\\n[ì½”ë“œë¸”ë¡]N[ì½”ë“œë¸”ë¡]\\n\\nì€ ì „ì²´ ë¬¸ì„œ ìˆ˜"

**CRITICAL - ì½”ë“œ ë¸”ë¡ ì‚¬ìš© ê¸ˆì§€**:
- ì ˆëŒ€ ì‚¼ì¤‘ ë°±í‹± ì½”ë“œ ë¸”ë¡ì„ ì‚¬ìš©í•˜ì§€ ë§ˆë¼
- ìˆ˜ì‹ì€ $$ìˆ˜ì‹$$ (LaTeX)ë‚˜ ì¸ë¼ì¸ ì½”ë“œë§Œ ì‚¬ìš©
- N, n_t ê°™ì€ ë³€ìˆ˜ë¥¼ ê°ê° ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ìª¼ê°œì§€ ë§ˆë¼

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:

{
  "slides": [
    {
      "title": "ìŠ¬ë¼ì´ë“œ ì œëª©",
      "content": "**í•µì‹¬ ê°œë…**ì€ ì´ë ‡ë‹¤.\\n\\n*ì¤‘ìš”í•œ ë¬¸ì¥*ì„ ê°•ì¡°í•˜ê³ , \`ë³€ìˆ˜\`ëŠ” ì½”ë“œë¡œ.\\n\\n> ğŸ’¡ **í•œ ê±¸ìŒ ë”**: ì¶”ê°€ ì„¤ëª…...",
      "keyPoints": [
        "ì‹œí—˜ì— ë‚˜ì˜¬ í•µì‹¬ í¬ì¸íŠ¸ 1 (ê³µì‹, ì •ì˜, ê°œë…)",
        "ì‹œí—˜ì— ë‚˜ì˜¬ í•µì‹¬ í¬ì¸íŠ¸ 2 (ì ìš© ë°©ë²•)",
        "ì‹œí—˜ì— ë‚˜ì˜¬ í•µì‹¬ í¬ì¸íŠ¸ 3 (ì£¼ì˜ì‚¬í•­ì´ë‚˜ í•¨ì •)"
      ]
    }
  ]
}

**ë§ˆì§€ë§‰ 2í˜ì´ì§€ (í•„ìˆ˜!)**:
- **page N-1**: "í•µì‹¬ ê°œë… ì´ì •ë¦¬" - ì „ì²´ ë‚´ìš©ì˜ í•µì‹¬ ê°œë…ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬
- **page N**: "ì‹œí—˜ ëŒ€ë¹„ ìš”ì•½" - ì‹œí—˜ì— ê¼­ ë‚˜ì˜¬ ë‚´ìš©ë§Œ ì••ì¶• ì •ë¦¬

**ì¤‘ìš”**:
- contentëŠ” "í•µì‹¬ê°œë…:", "ì‹œí—˜í¬ì¸íŠ¸:" ê°™ì€ ë¼ë²¨ ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±
- ì¤‘ìš”í•œ ê°œë…ì€ **ë°˜ë“œì‹œ** **êµµê²Œ** í‘œì‹œ
- ë¬¸ë‹¨ ì‚¬ì´ ë¹ˆ ì¤„(\\n\\n) í•„ìˆ˜`
          : `ë‹¹ì‹ ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ì—…ë¬´ ìë£Œë¥¼ ì£¼ì œë³„ë¡œ ì••ì¶•í•œ ${compressedSummaries.length}ê°œì˜ Topic ìš”ì•½ì…ë‹ˆë‹¤.

${compressedSummaries.join('\n\n====================\n\n')}

**ì„ë¬´**: ì´ Topic ìš”ì•½ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì—…ë¬´ìš© ìµœì¢… ìŠ¬ë¼ì´ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.

**ëª©í‘œ**:
- ì „ì²´ ë‚´ìš©ì„ 12-18ê°œ ìŠ¬ë¼ì´ë“œë¡œ ì••ì¶• (ë§ˆì§€ë§‰ 2í˜ì´ì§€ëŠ” ì „ì²´ ìš”ì•½)
- ê° ìŠ¬ë¼ì´ë“œëŠ” í•˜ë‚˜ì˜ ì—…ë¬´ í”„ë¡œì„¸ìŠ¤ë‚˜ ì£¼ì œë¥¼ ë‹¤ë£¸
- ì‹¤ë¬´ì— í™œìš© ê°€ëŠ¥í•œ í•µì‹¬ ë‚´ìš©ë§Œ í¬í•¨
- Topicê°„ ë…¼ë¦¬ì  íë¦„ ìœ ì§€

**content ì‘ì„± ê·œì¹™** (ë§¤ìš° ì¤‘ìš”!):
1. **ì™„ê²°ëœ ë¬¸ì¥/ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±** - ì ˆëŒ€ "ë¡œ, ì—¬ê¸°ì„œ", "ì€ ~", "ëŠ” ~" ê°™ì´ ì•ë’¤ê°€ ì˜ë¦° ë¬¸ì¥ì„ ë§Œë“¤ì§€ ë§ˆë¼
2. **í”„ë¡œì„¸ìŠ¤ ì„¤ëª…ì€ í•œ ë¬¸ë‹¨ì— ì™„ê²°ë˜ê²Œ** - ì ˆì°¨ë¥¼ ì„¤ëª…í•  ë•Œ í•œ ë¬¸ë‹¨ ì•ˆì—ì„œ ëª¨ë“  ë‹¨ê³„ë¥¼ í¬í•¨
3. **Markdown í˜•ì‹**: ë¬¸ë‹¨ êµ¬ë¶„ì„ ìœ„í•´ ë¹ˆ ì¤„(\\n\\n) ì‚¬ìš©
4. **ì¤‘ìš”í•œ í”„ë¡œì„¸ìŠ¤/ìš©ì–´ëŠ” **êµµê²Œ**** (ë³´ë¼ìƒ‰), **í•µì‹¬ ë¬¸ì¥ì€ *ê¸°ìš¸ì„*** (íŒŒë€ìƒ‰)
5. **ì½”ë“œ/ë³€ìˆ˜/ê¸°ìˆ ìš©ì–´ëŠ” ì¸ë¼ì¸ ì½”ë“œ í˜•ì‹** (ì£¼í™©ìƒ‰)
6. **ì¶”ê°€ ì„¤ëª…ì€ > ì¸ìš©êµ¬** (ì‹œì•ˆ ë¸”ë£¨ ì¹´ë“œ)

**ì˜¬ë°”ë¥¸ ì˜ˆì‹œ**:
"ì´ **í”„ë¡œì„¸ìŠ¤ ë³€ê²½ì˜ í•µì‹¬**ì€ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥ì— ìˆë‹¤.\\n\\n*ê¸°ì¡´ A ë°©ì‹ì€ ë™ì‹œì„± ì²˜ë¦¬ì—ì„œ race conditionì´ ë°œìƒí–ˆë‹¤.* B ë°©ì‹ì€ \`transaction isolation level\`ì„ \`READ COMMITTED\`ì—ì„œ \`SERIALIZABLE\`ë¡œ ì¡°ì •í•´ ì´ ë¬¸ì œë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ í•´ê²°í•œë‹¤.\\n\\n> âš ï¸ **ì‹¤ë¬´ ì£¼ì˜**: ë°˜ë“œì‹œ \`lock_timeout\`ê³¼ \`deadlock_timeout\` ê°’ì„ ëª¨ë‹ˆí„°ë§í•´ì•¼ í•œë‹¤. ê¸°ë³¸ê°’ 30ì´ˆë¡œëŠ” ë¶€ì¡±í•  ìˆ˜ ìˆìœ¼ë©°, í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” 60ì´ˆ ì´ìƒ ê¶Œì¥í•œë‹¤."

**ì˜ëª»ëœ ì˜ˆì‹œ (ì ˆëŒ€ í•˜ì§€ ë§ˆë¼)**:
"[ì½”ë“œë¸”ë¡]transaction isolation level[ì½”ë“œë¸”ë¡]\\n\\nì„ ì¡°ì •í•˜ëŠ”ë°,\\n\\n[ì½”ë“œë¸”ë¡]READ COMMITTED[ì½”ë“œë¸”ë¡]\\n\\nëŠ” ê¸°ë³¸ê°’"

**CRITICAL - ì½”ë“œ ë¸”ë¡ ì‚¬ìš© ê¸ˆì§€**:
- ì ˆëŒ€ ì‚¼ì¤‘ ë°±í‹± ì½”ë“œ ë¸”ë¡ì„ ì‚¬ìš©í•˜ì§€ ë§ˆë¼
- ê¸°ìˆ  ìš©ì–´ëŠ” ì¸ë¼ì¸ ì½”ë“œë§Œ ì‚¬ìš©
- ìš©ì–´ë¥¼ ê°ê° ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ìª¼ê°œì§€ ë§ˆë¼

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:

{
  "slides": [
    {
      "title": "ìŠ¬ë¼ì´ë“œ ì œëª©",
      "content": "**í•µì‹¬ í”„ë¡œì„¸ìŠ¤**ëŠ” ì´ë ‡ë‹¤.\\n\\n*ì¤‘ìš”í•œ ë¬¸ì¥*ì„ ê°•ì¡°í•˜ê³ , \`ì½”ë“œ\`ëŠ” ì´ë ‡ê²Œ.\\n\\n> âš ï¸ **ì‹¤ë¬´ ì£¼ì˜**: ì¶”ê°€ ì„¤ëª…...",
      "keyPoints": [
        "ì‹¤ë¬´ì— ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 1",
        "ì—…ë¬´ íš¨ìœ¨ì„ ë†’ì´ëŠ” í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 2",
        "ì£¼ì˜ì‚¬í•­ì´ë‚˜ Best Practice"
      ]
    }
  ]
}

**ë§ˆì§€ë§‰ 2í˜ì´ì§€ (í•„ìˆ˜!)**:
- **page N-1**: "í•µì‹¬ í”„ë¡œì„¸ìŠ¤ ì´ì •ë¦¬" - ì „ì²´ ì—…ë¬´ íë¦„ê³¼ í•µì‹¬ ë‚´ìš© ì²´ê³„ì  ì •ë¦¬
- **page N**: "ì‹¤ë¬´ ì ìš© ìš”ì•½" - ì‹¤ë¬´ì— ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ í•µì‹¬ë§Œ ì••ì¶• ì •ë¦¬

**ì¤‘ìš”**:
- contentëŠ” "í•µì‹¬ê°œë…:", "ì—…ë¬´í¬ì¸íŠ¸:" ê°™ì€ ë¼ë²¨ ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±
- ì¤‘ìš”í•œ í”„ë¡œì„¸ìŠ¤/ìš©ì–´ëŠ” **ë°˜ë“œì‹œ** **êµµê²Œ** í‘œì‹œ
- ë¬¸ë‹¨ ì‚¬ì´ ë¹ˆ ì¤„(\\n\\n) í•„ìˆ˜`;

        console.log(`[FINAL] Calling gpt-5.1 for final integration (SINGLE CALL)`);
        const finalCompletion = await openai.chat.completions.create({
          model: FINAL_MODEL,
          messages: [
            {
              role: "system",
              content: `You are a direct and efficient assistant.

${type === "exam" ? STUDY_SYSTEM_PROMPT : WORK_SYSTEM_PROMPT}`
            },
            { role: "user", content: finalPrompt }
          ],
          response_format: { type: "json_object" },
          temperature: 1.0,
          reasoning_effort: "low"  // Force disable reasoning tokens (5x cost reduction)
        });

        const finalData = JSON.parse(finalCompletion.choices[0].message.content || "{}");
        const slides = finalData.slides || [];
        console.log(`[FINAL] Generated ${slides.length} slides`);

        // Create page objects
        const allPages = slides.map((slide: any, idx: number) => ({
          page: idx + 1,
          title: slide.title || `ìŠ¬ë¼ì´ë“œ ${idx + 1}`,
          content: slide.content || "",
          keyPoints: slide.keyPoints || []
        }));

        // Save to DB
        await supabase
          .from("materials")
          .update({ analysis: { page_analyses: allPages } })
          .eq("id", materialId);

        // Send to frontend
        for (const page of allPages) {
          await sendEvent("page", page);
        }

        // Calculate final cost
        const finalInputCost = (finalCompletion.usage?.prompt_tokens || 0) * 0.000003;
        const finalOutputCost = (finalCompletion.usage?.completion_tokens || 0) * 0.000015;
        const finalCost = finalInputCost + finalOutputCost;
        const totalCost = totalBatchCost + summaryCost + finalCost;

        console.log(`[COST] Final gpt-5.1 call: $${finalCost.toFixed(4)} (input: $${finalInputCost.toFixed(4)}, output: $${finalOutputCost.toFixed(4)})`);
        console.log(`[COST] TOTAL: $${totalCost.toFixed(4)} (batch: $${totalBatchCost.toFixed(4)}, summary: $${summaryCost.toFixed(4)}, final: $${finalCost.toFixed(4)})`);

        // Send completion event
        await sendEvent("complete", { success: true, total_slides: slides.length });
      }

      await writer.close();
    } catch (error: any) {
      console.error("[STREAM ERROR]", error);
      await sendEvent("error", { error: "Analysis failed", details: error.message });
      await writer.close();
    }
  })();

  return new Response(stream.readable, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache",
      "Connection": "keep-alive",
    },
  });
}
